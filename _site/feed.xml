<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GwanSiu Blog</title>
    <description>在黑暗中前行，在黑暗中摸索，探寻未知的光明</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 30 Jun 2017 22:02:02 +0800</pubDate>
    <lastBuildDate>Fri, 30 Jun 2017 22:02:02 +0800</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>
    
      <item>
        <title>Contrasive Loss(对比损失函数)</title>
        <description>&lt;h2 id=&quot;1-whats-the-contrasive-loss-function&quot;&gt;1. What’s the Contrasive Loss function?&lt;/h2&gt;
&lt;p&gt;Contrasive Loss function is a dimemsion reduction technique. Constrasive loss function can drive the featurs mapping function to learn a mapping that projects high dimension features into its low dimension manifold. In this low dimension manifold, the sample of the same category would cluster together and part away from the sample of different categories.(对比损失函数是一种降维学习方法，它可以学习一种映射关系，这种映射关系可以使得在高维空间中，相同类别但距离较远的点，通过函数映射到低维空间后，距离变近，不同类别但距离都较近的点，通过映射后再低维空间变得更远。这样的结果就是，在低维空间，同一种类的点会产生聚类的效果，不同种类的mean会隔开。类似fisher降维，但fisher降维不具有out-of-sample extension的效果，不能对new sample进行作用)&lt;/p&gt;

&lt;h2 id=&quot;2-why-is-contrasive-loss-funtion-needed&quot;&gt;2. Why is contrasive loss funtion needed?&lt;/h2&gt;
&lt;p&gt;Conventionally, dimension reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that “similar” points in input space are mapped to nearby points on the manifold. The classical dimension reduction methods are Principle Component Analysis(PCA) and Multi-Dimensional Scaling(MDS). PCA is to project sample points into a subspace where the variance of sample points is minimized. MDS is to project samples into a subspace that is best to preserve the pairwise distances between input points.
Another dimension reduction methods are ISOMAP and Local-linear Embedding(LLE). All of above methods presuppose the the existence of a meaningful metric in input space and the computation process usually has three steps: 1.Identify a list of neighborhoods of each points. 2. A gram matrix is computed using this information. 3. the eigenvalue problem is solved for this matrix.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;None of these methods attempt to compute a function that could map a new, unknown data point without recomputing the entire embedding and with- out knowing its relationships to the training points.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-whats-the-advantage-of-conrtasive-loss-function&quot;&gt;3. What’s the advantage of conrtasive loss function?&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;It only needs neighborhood relationships between training samples. These relationships could come from prior knowledge, or manual labeling, and be indepen- dent of any distance metric.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Itmaylearnfunctionsthatareinvarianttocomplicated non-linear trnasformations of the inputs such as light- ing changes and geometric distortions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The learned function can be used to map new samples not seen during training, with no prior knowledge.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The mapping generated by the function is in some sense “smooth” and coherent in the output space.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;4-how-does-the-contrasive-loss-work&quot;&gt;4. How does the contrasive loss work?&lt;/h2&gt;

&lt;p&gt;Let’s consider a pair of sample $\vec{X_{1}},\vec{X_{2}}\in I$ Let $Y$ be a binary label assigned to this pair. $Y = 0$ if $\vec{X_{1}}$ and $\vec{X_{2}}$ are deemd similar, and $Y = 1$ if they are deemed dissimilar. Define the parameterized distance function to be learned DW between $\vec{X_{1}}$, $\vec{X_{2}}$ as the euclidean distance between the outputs of $G_{W}$ . That is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_{W}(\vec{X_{1}},\vec{X_{2}})=\text{||}G_{W}(\vec{X_{1}})-G_{W}(\vec{X_{2}}\text{||}_{2}&lt;/script&gt;

&lt;p&gt;The general form is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(W)=\sum_{i=1}^{P}L(W,(Y,\vec{X}_{1}, \vec{X}_{2})^{i})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(W,(Y,\vec{X_{1}}, \vec{X_{2}})^{i})=(1-Y)L_{S}(D_{W}^{i})+YL_{D}(D^{i}_{W})&lt;/script&gt;

&lt;p&gt;where $(Y,\vec{X}&lt;em&gt;{1}, \vec{X}&lt;/em&gt;{2})^{i})$ is the i-th labeled sample pair, $L_{S}$ is the partial loss function for a pair of similar points, $L_{D}$ the partial loss function for a pair of dissimilar points, and P the number of training pairs(which may be as large as the square of the number of samples).&lt;/p&gt;

&lt;p&gt;$L_{S}$ and $L_{D}$ must be designed such that minimizing $L$ with respect to $W$ would result in low values of $D_{W}$ for similar pairs and high values of $D_{W}$ for dissimilar pairs.&lt;/p&gt;

&lt;p&gt;For example, the exact loss function is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(W,(Y,\vec{X_{1}}, \vec{X_{2}})^{i})=(1-Y)L_{S}(D_{W}^{i})+Y\frac{1}{2}\{\text{max}(0,m-D_{w})\}^{2}&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/fwzo5gzyyun344jbh90v0go8/image_1bjehi3lpq601crmvc5vk51ndd9.png&quot; alt=&quot;image_1bjehi3lpq601crmvc5vk51ndd9.png-51.8kB&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where $m&amp;gt;0$ is margin. The margin define a radius around $G_{W}(\vec{X})$.
Disimilar pairs contribute to the loss only if their distance is within the his radius.&lt;/p&gt;

&lt;h2 id=&quot;5-spring-model-explaination&quot;&gt;5 Spring model Explaination&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/8ur3ozxwcknsn6r4kedpp2b2/image_1bjehp2jo1n8mvtl1o861fh41iavm.png&quot; alt=&quot;image_1bjehp2jo1n8mvtl1o861fh41iavm.png-59.9kB&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Black points are the same category as the blue point while the while points are different category.&lt;/p&gt;

&lt;p&gt;图(a)，当同种类的点距离在$G_{W}(\vec{X})$圆外(即:距离较远)，Contrasive loss算法会将同种类的点往里推，使其都聚集在一个领域内。图(b),当不同种类的点距离在$G_{W}(\vec{X})$圆内(即:距离较近)，Contrasive loss算法会将同种类的点往外推，使其都聚集在一个领域内。这样造成的效果就是，园内和圆外分别就形成了同一种类的cluster。图(e)是指contrasive loss算法的最终形态，是达到不同种类点之间的动态平衡，从而使loss最小。&lt;/p&gt;

&lt;h2 id=&quot;6-visual-graph&quot;&gt;6 Visual Graph&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/ewdd427iz7lnqxq8ospmxlts/image_1bjei9scl9eu3oaf5n9651eqp9.png&quot; alt=&quot;image_1bjei9scl9eu3oaf5n9651eqp9.png-301.3kB&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/fygo87srhmpqwqmhwl18si6s/image_1bjeiart115p1gfp164i1372vokm.png&quot; alt=&quot;image_1bjeiart115p1gfp164i1372vokm.png-309.3kB&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/g3485tg7gkpvz8k43rzyh2f5/image_1bjeibb4o1jj61o5e17fpupr11pt13.png&quot; alt=&quot;image_1bjeibb4o1jj61o5e17fpupr11pt13.png-109.3kB&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 24 Jun 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/06/24/Contrasive-Loss/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/06/24/Contrasive-Loss/</guid>
        
        <category>Machine Learning</category>
        
        
      </item>
    
      <item>
        <title>Batch Normalization</title>
        <description>&lt;h2 id=&quot;1-why-batch-normalization&quot;&gt;&lt;strong&gt;1. Why Batch normalization?&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;batch normalization is normalization strategy, which makes the distribution of layers’ input consistent at the output of layers. Citate in paper: &lt;strong&gt;Batch normalization eliminate the effect of internal covariant shift.(&lt;/strong&gt;What’s internal covariate shift?** The input distribution to a learning system changes during training.) Covariate shift would amplify permutation at the deeper layers. If it happens, the inputs of activation function will stay in the saturated region. In that region, the gradient will be very small and the phenomenon of vanishing gradient would happen and stop neural network to train.&lt;/p&gt;

&lt;p&gt;Therefore, the batch normalization layer is to force the distribution of layers’ input to remain fixed over training time. Without batch normalization, sometimes the neural network used ReLU activation function still works with careful initialization and small learning rate. This is because neural network will put more effort to compensate for the changes in the distribution. Usually, it would degrade the performance of neural network.&lt;/p&gt;

&lt;h2 id=&quot;2-the-diagram-of-batch-normalization&quot;&gt;2. The diagram of batch normalization&lt;/h2&gt;
&lt;h3 id=&quot;21-the-batch-normalization-diagram&quot;&gt;2.1 The Batch normalization diagram:&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/0t599cg5f5hlecjre84uu07w/image_1bileg6o33eejqeqqf4hhl4j9.png&quot; alt=&quot;image_1bileg6o33eejqeqqf4hhl4j9.png-66.5kB&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;22-bn-train-and-updated&quot;&gt;2.2 BN train and updated&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/lazgr3fyn2n1mt2euj72mnp3/image_1bilej2hv19ci158jrdnlt41p8km.png&quot; alt=&quot;image_1bilej2hv19ci158jrdnlt41p8km.png-51.2kB&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-the-place-of-batch-normalization&quot;&gt;3. The place of batch normalization&lt;/h2&gt;

&lt;p&gt;Generally, Batch normalization layer can apply to any input of layers. In this paper, batch normalization layers is added immediately before the activation layers. Condsider  a neural network consists of an affine transformation followed by the element-wise nonlinearity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z = g(W\mu+b)&lt;/script&gt;

&lt;p&gt;where &lt;strong&gt;W&lt;/strong&gt; and &lt;strong&gt;b&lt;/strong&gt; are learned parameters of the model, and $g(·)$ is the nonlinearity such as sigmoid or ReLU.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why not the input of the layers x?&lt;/strong&gt;  because $\mu$ is likely the output of another nonlinearity, the shape of its distribution is likely to change during training, and constraining its first and second moments would nonsense and not eliminate the covariate shift. The same  principle to the convolutional layer, it means batch normalization layer is inserted between convolutional layer and the activation layer.&lt;/p&gt;

&lt;h2 id=&quot;4-the-benefits-of-the-batch-normalization&quot;&gt;4. The benefits of the batch normalization&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;increase learning rate.&lt;/li&gt;
  &lt;li&gt;Remove the dropout and reduce the L2 weight regularization in some sense, because the batch normalization layer could regulate the neural network.&lt;/li&gt;
  &lt;li&gt;Accelerate the learning rate decay.&lt;/li&gt;
  &lt;li&gt;Remove the local response normalization.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 06 Jun 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/06/06/BN/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/06/06/BN/</guid>
        
        <category>Machine Learning</category>
        
        
      </item>
    
      <item>
        <title>DeepID系列</title>
        <description>&lt;h2 id=&quot;1introduction&quot;&gt;1.Introduction&lt;/h2&gt;
&lt;p&gt;Face recognition contains identification and verification. Identification is to classify an input image into a large number of identity classes, when verification is to classify a pair of images as belonging to the same identity or not.(i.e. binary classification). Identification is more challenging than verification. Because it’s more difficult to predict a training sample into one of many classes than to perform binary classification.&lt;/p&gt;

&lt;p&gt;The basic philosophy of face recognition is to maximize inter-personal variations and minimize intra-personal variations. For example, fisher face approach is achieved this target. Another method is metric learning, which is to maps faces to some representations such that face of same identity is close to each other while those of different identities stay apart.&lt;/p&gt;

&lt;h2 id=&quot;2deepid&quot;&gt;2.DeepID&lt;/h2&gt;
&lt;p&gt;In most face recognition algorithms, human face represents over-complete low-level feature based on shallow model. In DeepID, ConvNet takes a face patch as input and extracts high-level feature to represent human face. 200+ ConvNets(each ConvNets are corresponding to one patch) are utilized to train  for identification, and then used trained ConvNet to extract face features and are feeded into join bayesian model for verification.&lt;br /&gt;
The arichitecture of DeepID ConvNets is followed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/w67416fln6h1l4u9dshzdg8w/image_1bhupd992uhqupgtld1d571c6l9.png&quot; alt=&quot;image_1bhupd992uhqupgtld1d571c6l9.png-46.4kB&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The properties of DeepID:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The last hidden layer of DeepID is fully connected to both the third and fourth convolutional layers (after max- pooling) such that it sees multi-scale features. This is critical to feature learning because after successive down-sampling along the cascade, the fourth convolutional layer contains too few neurons and becomes the bottleneck for information propagation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Feature numbers continues to reduce along the feature extraction hierachy until the last hidden layer(the DeepID layer).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Weights in higher convolutional layers of our ConvNets are locally shared to learn different mid- or high-level features in different regions.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;How does the DeepID extract features for identity task?&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Highly compact features(160 dimensions in this paper) are extracted in the top layer from the alternatively stack of convolutional layers. &lt;strong&gt;why to do so?&lt;/strong&gt; Because it contrains DeepID to be significantly fewer than the classes of identities they predict, which is the key to learning highly compact and discriminative. It implicitly adds a strong regulatization to ConvNets, which helps to form shared hidden representations that can classify all the identities well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;weights in higher convolutional layer of our ConvNets are locally shared to learn differnent mid- or high-level feature in different regions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The last hidden layer of DeepID is fully connected to both the third and fourth convolutional layers(after max-pooling) such that it sess multi-scale features.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;deepid-2&quot;&gt;DeepID 2&lt;/h2&gt;
&lt;p&gt;In DeepID2, supervisory signal is added to reduce the intra-personal variation. The basic idea of face recognition is to maximize inter-personal variation and minimize intra-personal variation at the same time. For DeepID, multi-classes classification(identificaiton, cross-entropy at the top layer) is to reduce the inter-personal variations. In DeepID2, verification signal is coorperated with the identificaiton signal to reduce the intra-personal variations.&lt;/p&gt;

&lt;p&gt;The structure of neural network is the same as the DeepID, the main difference is that verification signal is added. Verification signal can be thought as regulatization, which regularizes the DeepID features to reduce the intra-personal variations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/w67416fln6h1l4u9dshzdg8w/image_1bhupd992uhqupgtld1d571c6l9.png&quot; alt=&quot;image_1bhupd992uhqupgtld1d571c6l9.png-46.4kB&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For identificaiton, the loss function is cross-entropy:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Ident}(f,t,\theta_{id}) = - \sum^{n}_{i=1}p_{i}\text{log}\hat{p}_{i}=-\text{log}\hat{p}_{t}&lt;/script&gt;

&lt;p&gt;In this paper, author adopts two verification loss. One loss function is based on $L_{2}$ norm proposed by Hadesll. Another loss function is based on the cosine similarity.&lt;/p&gt;

&lt;p&gt;$L_{2}$ norm loss function:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\text{Verif}(f_{i}, f_{j},y_{ij},\theta_{ve})=\begin{cases}
\frac{1}{2}\vert \vert f_{i}-f_{j}\vert \vert^{2}, &amp;\text{if } y_{ij}=1\cr \frac{1}{2}\text{max}(0,m-\vert \vert f_{i}-f_{j}\vert \vert_{2})^{2}, &amp;\text{if }y_{ij}=-1 \end{cases} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where $f_{i}$ and $f_{j}$ are DeepID2 features vectors extracted from the two face images in comparision. $y_{ij}=1$ means that $f_{i}$ and $f_{j}$ are the same same person, while $y_{ij}=-1$ means that $f_{i}$ and $f_{j}$ are different person.$m$ is the distance margin. $\theta_{ve}$ is the verification loss parameters that can be learned in the training processing.&lt;/p&gt;

&lt;p&gt;The loss function based on the cosine similarity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Verif}(f_{i},f_{j},y_{ij},\theta_{ve}) = \frac{1}{2}(y_{ij}-\sigma(\omega d+b))^{2}&lt;/script&gt;

&lt;p&gt;where $d = \frac{f_{i}\bullet f_{j}}{\vert \vert  f_{i}\vert \vert_{2} \vert \vert f_{j} \vert \vert_{2}}$ is the cosine similarity between DeepID2 feature vectors, $\theta_{ve} = {\omega,b}$ are learnable scaling and shifting  arameters, $\sigma$ is the sigmoid function, and $y_{ij}$ is the binary target of whether the two compared face images belong to the same identity.&lt;/p&gt;

&lt;p&gt;The goal is to learn the parameters $\theta_{c}$ in the feature extraction function Conv(·), while $\theta{id}$ and $\theta{ve}$ are only parameters introduced to propagate the identification and verification signals during training. All the parameters are undated by the gradient descent algorithms.
In the testing stage, only $\theta_{c}$ are used to extract the features. $m$ is the loss function based on $L_{2}$ norm can’t be updated snice it callopses to zero. The algotithms is as followed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/eqvotpb5s4ee285f7bujxx6g/image_1bi8un324tir1pko32d15k616r39.png&quot; alt=&quot;image_1bi8un324tir1pko32d15k616r39.png-156kB&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s the mean of $\lambda$?&lt;/strong&gt;
Identification signal and Verification signal are changed when $\lambda$ is varied from $0$ to $\infty$. At $\lambda = 0$, the verification signal vanishes and only the identification signal takes effect. When $\lambda$ increases, the verification signal gradually dominates the training process. At the other extreme of $\lambda \rightarrow \infty$, only the verification signal remains.&lt;/p&gt;

&lt;h2 id=&quot;deepid2&quot;&gt;DeepID2+&lt;/h2&gt;

&lt;p&gt;Compared with the DeepID 2, DeepID2+ added the supervisory signal in the early layers and increases the dimension of hidden repsresentation. In the DeepID 2+,Aauthor discover some nice property of neural network: sparsity, selecvtivity and robustness. The structure of network is as followed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/mqz44xsil7ngw2rxm2anyekz/image_1bi91dna91qjjuant2f1uu1marm.png&quot; alt=&quot;image_1bi91dna91qjjuant2f1uu1marm.png-179.8kB&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s sparsity mean?&lt;/strong&gt; It’s observed that neural activiation is sparse, and moderate sparsity can maximizes the discriminative power of deep neural network and increase the inter-personal distance. Therefore, DeepID2+ still acieve high performance after sparing the feature.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s the selectivity mean?&lt;/strong&gt; It’s neurons in hidden layers are highly selective to the identity or identity-related attributes. The figure is shown this property.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/c5eec4i77zjk2leba46vk8p1/image_1bi91aveo1e8m2r4rv4roq1tvj9.png&quot; alt=&quot;image_1bi91aveo1e8m2r4rv4roq1tvj9.png-237.4kB&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;deepid-3&quot;&gt;DeepID 3&lt;/h2&gt;

&lt;p&gt;In DeepID3, author investigated how very deep structure of neural work influences the performace. In DeepID 3, it proposes two very deep neural networks one is to stacke convolutional layers and another one stacks inception layers at the top several layres instead of the convolutional layers.&lt;/p&gt;

&lt;p&gt;The structure of DeepID 3 is as followed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/1lru1luh4aj9398wj36qlh6y/image_1bi921k77cme17tu19ct12em1qs313.png&quot; alt=&quot;image_1bi921k77cme17tu19ct12em1qs313.png-143kB&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 06 Jun 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/06/06/DeepID/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/06/06/DeepID/</guid>
        
        <category>Machine Learning</category>
        
        
      </item>
    
      <item>
        <title>Note--Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</title>
        <description>&lt;h2 id=&quot;1-introduction-and-background&quot;&gt;1. Introduction and background&lt;/h2&gt;

&lt;p&gt;Conventionally, deep neural network requires for fixed-size input. In some applications, such as recognition and detection, input images are usually cropped and warped, and then feeded into the deep neural network. Crop operator can’t obtain the whole object which means crop may lead to some information loss in some sense, and warp operator would introduce unwanted geometric distortion. These limitations will harm the final performance of neural network. In the paper of «Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition», kaiming He propose a new pooling strategy–Spatial pyramid pooling, which borrow the idea from the spatial pyramid matching model(SPM[2]). The outstanding contribution of this struture is to generate fiex-length output regardless of the input size, while previous networks can’t.&lt;/p&gt;

&lt;p&gt;The comparision of SPP-net and conventional net:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/8x0a4pd8k6z1uj2eqyymuihi/image_1bhbp36ic1lku1s5f1hlbodh15ll9.png&quot; alt=&quot;image_1bhbp36ic1lku1s5f1hlbodh15ll9.png-169.6kB&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-spatial-pyramid-pooling-architecture&quot;&gt;2. Spatial pyramid pooling architecture&lt;/h2&gt;

&lt;h3 id=&quot;21-why-does-the-cnn-requires-for-fixed-size-input&quot;&gt;2.1 Why does the CNN requires for fixed-size input?&lt;/h3&gt;

&lt;p&gt;CNN consists of two parts: convolutional layer and fully-connected layer which is usually at the top of neural network. The convolutional layers operate in a sliding window manner and output feature map which represents the arangement of the activations. In fact, the convolutional layers doesn’t require fixed-size input and can generate feature map of any size. Note that ** the size here means the width and height of feature map not the channel number. As we all know, channel number is fixed given the filer number. &lt;br /&gt;
On the other hand, fully-connected layer need fixed-size input. In detail, fully-connected layers essentially is linear projection operator, dot product.fFrom the above analysis, fully-connected constraint the input size of neural network. Kaiming He deal with this problem by inserting the SPP-net between convolutional layers and fully-connected layers.&lt;/p&gt;

&lt;h3 id=&quot;22-whats-the-spp-net-architecture-and-how-does-it-work&quot;&gt;2.2 What’s the SPP-net architecture and how does it work?&lt;/h3&gt;

&lt;p&gt;The structure of SPP-net is as followed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/6ky8a9fsgltk6bgf2326h41c/image_1bhbqlv45ad3181t1mgnsj4b6m.png&quot; alt=&quot;image_1bhbqlv45ad3181t1mgnsj4b6m.png-122.3kB&quot; /&gt;&lt;/p&gt;

&lt;p&gt;SPP-net is inserted into between convolutional layers and fully-conneccted layers. In SPP-net, it adopts spatial pyramid structure, in other word, it divided input feature into subimage and extract patch feature in each subimage. Kaiming He adopt max pooling strategy in this paper, it means the maximum value in each subimage would be extrected.&lt;/p&gt;

&lt;p&gt;As we all know, the sliding window manner in convolutional layers densely extrect image patches, which is very effective for feature representation. The spatial pyramid pooling is a local-area operator. Spatial pyramid pooling improves BoW in that it can maintain spatial information by pooling in local spatial bins. &lt;strong&gt;It can genrate fixed-sized output regardless of inout size, which means the the scale of image doesn’t affect the final performance, it would extrect scale-invariant feature.&lt;/strong&gt; The scales are also important for the accuracy of deep networks.&lt;/p&gt;

&lt;p&gt;As we can see from the figure, the coarsest pyramid level has a single bin tha tcovers the ertire image. This essentially is a “global pooling operation”. In the paper of “network in network”, in[3],a global average pooling is used to reduce the model size and also reduce overfitting; A global average pooling is used on the testing stage after all fc layers to improve accuracy; in [4], a global max pooling is used for weakly supervised object recognition.&lt;/p&gt;

&lt;p&gt;The adavantage of SPP-net:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;SPP is able to generate a fixed- length output regardless of the input size, while the sliding window pooling used in the previous deep networks cannot.&lt;/li&gt;
  &lt;li&gt;SPP uses multi-level spatial bins, while the sliding window pooling uses only a single window size. Multi-level pooling has been shown to be robust to object deformations.&lt;/li&gt;
  &lt;li&gt;SPP can pool features extracted at variable scales thanks to the flexibility of input scales.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;3multi-view-testing-used-by-spp&quot;&gt;3.Multi-view testing used by SPP&lt;/h2&gt;
&lt;p&gt;Thanks to the flexibility of SPP, kaiming he propose a multi-view method used by SPP-net in this paper. SPP-net can easily extract from the windows of the arbitrary size. It’s descriped that assume the image is resized to min(h,s)=s where s is predefined scale(like 255). The convolutional feature maps are computed from the entire image through convolutional layers. For the usage of
flipped views, we also compute the feature maps of the flipped image. Given any view (window) in the image, we map this window to the feature maps (the way of mapping is in Appendix), and then use SPP to pool the features from this window (see Figure 5). The pooled features are then fed into the fc layers to compute the softmax score of this window. These scores are averaged for the final prediction. &lt;br /&gt;
&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/j2r33c1s45egd98s819cyhb0/image_1bhc39n0frdd1up8fua1qf4bvi13.png&quot; alt=&quot;image_1bhc39n0frdd1up8fua1qf4bvi13.png-144.7kB&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;br /&gt;
[1] K.He,X.Zhang,S.Ren,andJ.Sun.Spatialpyramidpoolingindeep
convolutional networks for visual recognition. In ECCV, 2014.&lt;br /&gt;
[2] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of fea- tures: Spatial pyramid matching for recognizing natural scene categories,” in CVPR, 2006.&lt;br /&gt;
[3] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” arXiv:1409.1556, 2014&lt;br /&gt;
[4] M. Oquab, L. Bottou, I. Laptev, J. Sivic et al., “Learning and transferring mid-level image representations using convolu- tional neural networks,” in CVPR, 2014.&lt;/p&gt;

</description>
        <pubDate>Sat, 27 May 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/05/27/SPP/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/05/27/SPP/</guid>
        
        <category>Machine Learning</category>
        
        
      </item>
    
      <item>
        <title>SIFT算子</title>
        <description>&lt;h2 id=&quot;1introduction&quot;&gt;1.Introduction&lt;/h2&gt;
&lt;p&gt;SIFT(Scale Invariant Feature Transform) featuers are invariant to image scaling and ratation, and patially invariant to change in illumination and 3D camera viewpoint. The key point of this approach is to generate lots of numbers of features and densely cover the image over all the location and scale.&lt;/p&gt;

&lt;p&gt;The properties of SIFT features:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;SIFT features are invariant to image scaling and roration, and patially invariant to change in illumination and 3D camera viewpoint. The featues are also robust to illuminatin, occlusion and affine transform.&lt;/li&gt;
  &lt;li&gt;SIFT features are highly distinctive, which allows a single feature to be correctly matched with high probability against a large database of features, providing a basis for object and scene recognition.&lt;/li&gt;
  &lt;li&gt;lots of numbers of features can be generaed from few objects.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Real-Time&lt;/strong&gt;: For object recognition, SIFT features can be matched quickly in large database.&lt;/li&gt;
  &lt;li&gt;SIFT also have the the ability to detect small objects in cluttered backgrounds requires that at least 3 features be correctly matched from each object for reliable identification.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this blog, we simply summary SIFT featues and its properties. In section 2, the produce of SIFT features caculation is introduced. In section 3, we will analysis SIFT features in depth and try to answer the question: why are SIFT features invariant to image scaling and rotation? why are SIFT featues robust to illumination? In addition, this blog is written in English and Chinese.&lt;/p&gt;

&lt;h2 id=&quot;2the-process-of-sift-calculation摘抄原文1&quot;&gt;2.The Process of SIFT calculation(摘抄原文[1])&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Scale-space extrema detection（尺度空间的极点检测）:&lt;/strong&gt;The first stage of computation searches over all scales and image locations. It is implemented efficiently by &lt;em&gt;using a difference-of-Gaussian function to identify potential interest points that are invariant to scale and orientation.&lt;/em&gt; （所有图像所有尺度和位置，使用difference-of-Gaussian function检测出具有尺度不变性和方向不变性的interst points）&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Keypoint localization(特征点定位):&lt;/strong&gt; At each candidate location, a detailed model is fit to determine location and scale. &lt;em&gt;Keypoints are selected based on measures of their stability.&lt;/em&gt; (这一步是在所有的特征点中，选择出具有稳定性的特征点)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Orientation assignment(赋予方向):&lt;/strong&gt; One or more orientations are assigned to each keypoint lo- cation based on local image gradient directions. &lt;strong&gt;All future operations are performed on image data that has been transformed relative to the assigned orientation, scale, and location for each feature, thereby providing invariance to these transformations.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Keypoint descriptor(特征点描述子):&lt;/strong&gt; The local image gradients are measured at the selected scale in the region around each keypoint. These are transformed into a representation that allows for significant levels of local shape distortion and change in illumination.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;21-scale-space-extrema-detection&quot;&gt;2.1 Scale-space extrema detection&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Construct image scale space based on Gaussian function:&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(x,y,\sigma)=G(x,y,\sigma)*I(x,y)&lt;/script&gt;

&lt;p&gt;where, $\text{*}$ is the convolution operation in x and y, and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G(x,y,\sigma) = \frac{1}{2\pi \sigma^{2}}e^{-(x^{2}+y^{2})/2\sigma^{2}}&lt;/script&gt;

&lt;p&gt;Using scale-space extrema in the difference-of-Guassian function convolved with the image to efficiently detect stable keypoint locations in the scale space.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
D(x,y,\sigma) &amp;=(G(x,y,k\sigma)-G(x,y,\sigma))* I(x,y) \\  

&amp;= L(x,y,k\sigma)-L(x,y,\sigma)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/0hz6rz3rpvdq1a39zsfvu5tm/image_1bh7i445eip13np1j0gv52m1e9.png&quot; alt=&quot;image_1bh7i445eip13np1j0gv52m1e9.png-88.1kB&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One octave represents one resolution of an image. In each octave,
different scale-space of the image are constructed  by the gaussian function. Adjacent Gaussian images are subtracted to produce the difference-of-Gaussian images on the right. After each octave, the Gaussian image is down-sampled by a factor of 2, and the process repeated.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. 为什么要使用gaussian function 来构造尺度空间？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Koenderink (1984) and Lindeberg (1994)在其论文中表明：Gasussian function 是唯一一个尺度空间核（scale-space kernel), 可以使用Gaussian function来构造图像的尺度空间。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2.为什么使用difference-of-gaussian function作为scale invariant features detector?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Difference-of-Gaussian function approximates to the the scale-normalized gaussian, $\sigma^{2}\nabla^{2}G$. Lindeberg (1994)研究显示：scale-normalized Laplacian of Gaussian, $\sigma^{2}\nabla^{2}G$,具有尺度不变性，and the maximum and minimum of the $\sigma^{2}\nabla^{2}G$ can produce the most stable image features compared to a range of other possible image funcitons, such as gradient, Hessian, or Harris corner function.&lt;br /&gt;
The relationship between $D$ and $\sigma\nabla^{2}G$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial G}{\partial \sigma} = \sigma \nabla^{2}G&lt;/script&gt;

&lt;p&gt;而: $\sigma \nabla^{2}G = \frac{\partial G}{\partial \sigma}\approx \frac{G(x,y,k\sigma)-G(x,y,\sigma)}{k\sigma - \sigma}$&lt;/p&gt;

&lt;p&gt;因此: $G(x,y,k\sigma)-G(x,y,\sigma) \approx (k-1)\sigma^{2}\nabla^{2}G$&lt;/p&gt;

&lt;p&gt;可见: difference-of-gaussian function 与 scale-normalized gaussian function 相差一个$\sigma^{2}$的尺度因子，而（k-1）是个常数，不影响极值点的位置。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;local extrema detection
&lt;strong&gt;how to locate local extrema（including local maxima and  minima）?&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Each sample point is compared to its eight neighbors in the current image and nine neighbors in the sccale above and below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/dpifzu907hxhj0thznbg3mvv/image_1bh8un05u16931pklgkl16blho4m.png&quot; alt=&quot;image_1bh8un05u16931pklgkl16blho4m.png-18.5kB&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;22-keypoint-localization&quot;&gt;2.2 Keypoint localization&lt;/h3&gt;
&lt;h4 id=&quot;221-accurate-keypoint-localization&quot;&gt;2.2.1 Accurate keypoint localization&lt;/h4&gt;
&lt;p&gt;根据选出的candidate keypoints,使用模型进一步准确拟合出Keypoints的位置（即计算出Keypoints的偏移量），在[link]: http://blog.csdn.net/zddblog/article/details/7521424 “zddhub”的博客中提到：图片是离散空间，而离散空间中的极值点并不是真正的极值点，从离散空间中寻找连续空间中极值点的是连续插值直至收敛。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/x0fzdgysh55ncdwz14ydct05/image_1bh8vbp5u2ib6k3e8v34tq9d13.png&quot; alt=&quot;image_1bh8vbp5u2ib6k3e8v34tq9d13.png-37.3kB&quot; /&gt;&lt;/p&gt;

&lt;p&gt;文中采用的是Brown and Lowe,2002提出的泰勒二阶拟合的方法：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D(x) = D + \frac{\partial D^{T}}{\partial x} + \frac{1}{2} x^{T} \frac{\partial^{2} D}{\partial x^{2}}x&lt;/script&gt;

&lt;p&gt;令$D(x)$的导数为零可以求得sample point $x=(x,y,\sigma)^{T}$的偏移量：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec x = -\frac{\partial^{2} D}{\partial x^{2}}^{-1}\frac{\partial D}{\partial x}&lt;/script&gt;

&lt;p&gt;进一步求得keypoints的准确位置：
&lt;script type=&quot;math/tex&quot;&gt;D(\hat{x}) = D + \frac{1}{2}\frac{\partial D}{\partial x} \vec x&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why is the function value at the extremumm, $D(\hat{x})$ useful for rejecting unstable extrema with low constrast.&lt;/strong&gt;
未填&lt;/p&gt;

&lt;p&gt;####2.2.2 Eliminating edge responses&lt;/p&gt;

&lt;p&gt;what’s the stability for the keypoint?
&lt;strong&gt;文中指出：for stability, it is not sufficient to reject keypoints with low constrast,The difference-of- Gaussian function will have a strong response along edges, even if the location along the edge is poorly determined and therefore unstable to small amounts of noise.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;并且 &lt;strong&gt;A poorly defined peak in the difference-of-Gaussian function will have a large principal curvature(曲率) across the edge but a small one in the perpendicular direction.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The priciple curvatures can be computed by the a $2x2$ Hessian matrix, $H$, is computed at the location and sccale of the keypoint:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
H=\left[
\begin{array}
 DD_{xx} &amp; D_{xy} \\  
 D_{xy} &amp; D_{yy} \\ 
\end{array}  
\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;文中指出：The eigenvalues of $H$ are proportional to the principal curvatures of $D$.&lt;/strong&gt;  但文中并不直接计算Hessian matrix $H$ 的特征值，而是通过计算最大特征值$\alpha$和次大特征值$\beta$的比值:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Tr(H) &amp;= D_{xx} + D_{yy} = \alpha + \beta \\
Det(D) &amp;= D_{xx}D_{yy} - (D_{xy})^{2} = \alpha \beta
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;虽然某一点的曲率可以取正负，但经过local extrema步骤筛选之后，not extrema is filtered, 因此$Det(H)$ 不太可能取负值。The ratio between the largest magnitude eigenvalue and the smaller one, let‘s $\alpha=r \beta$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{Tr(H)^{2}}{Det(H)} = \frac{(\alpha+\beta)^2}{\alpha\beta}=\frac{(r\beta+\beta)^{2}}{r\beta^{2}}=\frac{(r+1)^2}{r}&lt;/script&gt;

&lt;p&gt;which depends only on the ratio of the eigenvalues rather than their individual values. The quantity $(r + 1)^{2}/r$ is at a minimum when the two eigenvalues are equal and it increases with r. Therefore, to check that the ratio of principal curvatures is below some threshold, r, we only need to check&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{Tr(H)^{2}}{Det(H)} &lt; \frac{(r+1)^2}{r} %]]&gt;&lt;/script&gt;

&lt;p&gt;通过直接计算最大特征值$\alpha$和次大特征值$\beta$的比值，使得运算非常快捷。下图展示了r=10消除principal curvatures大于10的,效果图（c）–&amp;gt;(d)：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/wunk18g93vmoaefk3bn5bgdq/image_1bh92s2sdh01hdf4hl11p8jl09.png&quot; alt=&quot;image_1bh92s2sdh01hdf4hl11p8jl09.png-229.3kB&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;23-orientation-assignment&quot;&gt;2.3 Orientation assignment&lt;/h3&gt;
&lt;p&gt;给每个特征点赋予方向信息：&lt;strong&gt;利用关键点邻域像素的梯度方向分布特性为每个关键点指定方向参数，使算子具备旋转不变性[2]。&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
m(x,y) &amp;= \sqrt{(L(x+1,y)-L(x-1,y))^{2}+(L(x,y+1)-L(x,y-1))^2} \\
\theta (x,y) &amp;= \tan^{-1}((L(x,y+1)-L(x,y-1)))/((L(x+1,y)-L(x-1,y)))\\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;For each image, $L(x,y)$ at this scale, the gradient magnitude, m(x,y), and orientation, $\theta$.&lt;/p&gt;

&lt;h3 id=&quot;24-descriptor-representation梯度方向直方图128维向量&quot;&gt;2.4 Descriptor representation（梯度方向直方图，128维向量）&lt;/h3&gt;
&lt;p&gt;梯度方向直方图的范围是：0~360度，通常每十度为一个bin, 一共36个bin.也可以每45度为一bin,一共8bin。
随着距中心点越远的领域其对直方图的贡献也响应减小.Lowe论文中还提到要使用高斯函数对直方图进行平滑，减少突变的影响[2]。直方图的峰值代表着该处领域的主方向，其他bin中超过峰值80%的可以作为辅助方向。以下图为例：
&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/gams8f450qse4arlio8ggii5/image_1bh94cvdr1bgk1j2r9rc1c501tg5l.png&quot; alt=&quot;image_1bh94cvdr1bgk1j2r9rc1c501tg5l.png-92.4kB&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/4xywl7hn6igxye8rcb71hu6g/image_1bh94ql0n1ko512052hk1fu38615.png&quot; alt=&quot;image_1bh94ql0n1ko512052hk1fu38615.png-57.4kB&quot; /&gt;&lt;/p&gt;

&lt;p&gt;考虑keypoint为8x8的邻域，将其领域分成2x2的的区域（descriptor），每一个区域为4x4的subimage,在每一个subimage中做梯度方向直方图，每45度为一bin,一共8bin. 那么这个SIFT特征维度为：2x2x8=32. 文中实验考虑的是16x16的邻域，4x4的descriptor,8 bins,因此SIFT特征的维度为：4x4x8 = 128.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;最后一步则是将SIFT特征归一化，进一步消除光照的影响&lt;/strong&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 27 May 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/05/27/SIFT/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/05/27/SIFT/</guid>
        
        <category>Machine Learning</category>
        
        
      </item>
    
      <item>
        <title>Network in Network</title>
        <description>&lt;h3 id=&quot;content&quot;&gt;Content&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#1&quot;&gt;1.What’s the network in network? (An brief introduction)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2&quot;&gt;2.The arichitecture of the network in network&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3&quot;&gt;3.Why can the convolutional operator be replaced by a “micro-network”?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3.1&quot;&gt;3.1 The conventional neural network&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3.2&quot;&gt;3.2 why is the micro-network used?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3.3&quot;&gt;3.3 what’s the global average pooling operator&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3.4&quot;&gt;3.4 the overall structure of network in network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1whats-the-network-in-network-an-brief-introduction&quot;&gt;1.What’s the network in network? (An brief introduction)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Replace convolutional operator with a “micro-network”. In this article, the convolutional operator is considered as general operator and utilize multi-layer percetron as “micro-network”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Replace fully-connected layers with global average pooling layers. The purpose of fully-connected layers is to map a feature map to a vector for classification, but the fully-connected layers is prone to overfit. In tradictional arichitecture of CNN, the fully-connected layers is usually followed by the dropout layers to &lt;em&gt;prevent from overfitting&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2the-arichitecture-of-the-network-in-network&quot;&gt;2.The arichitecture of the network in network&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/ezwm4ibo080xpqzg2t5qtygu/image_1bghpbfup5jt7771oevlhiege9.png&quot; alt=&quot;image_1bghpbfup5jt7771oevlhiege9.png-94.7kB&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3why-can-the-convolutional-operator-be-replaced-by-a-micro-network&quot;&gt;3.Why can the convolutional operator be replaced by a “micro-network”?&lt;/h3&gt;

&lt;h4 id=&quot;31-the-conventional-neural-network&quot;&gt;3.1 The conventional neural network&lt;/h4&gt;
&lt;p&gt;The classical neural network is consist of alternatively the stack of convolutional filter and spatial pooling layers. The feature maps are generated by the conlutional filters which is followed by the activation functions, the feature map can be calculated as follows(linear rectifier function):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{i,j,k} = \text{max}(\omega^{T}_{k}x_{i,j},0)&lt;/script&gt;

&lt;p&gt;Here $(i,j)$ is the pixel index in the feature map, $x_{i,j}$ stands for the input patch centered at location $(i, j)$, and $k$ is used to index the channels of the feature map.&lt;/p&gt;

&lt;p&gt;The convolutional filter is considered as a general linear model for underlying local patch. Those convolutional filters can extracture invariant features from low level pixel. Indicidual linear filters can learn different vaiations of a same concept. The linear filters in the next layer will consider all combinations of variations from the previous layer.  At this point, we should note &lt;strong&gt;GLM can achieve a good extent of abstraction when the samples of the latent concepts are linearly separable, i.e. the variants of the concepts all live on one side of the separation plane defined by the GLM. The assuption behind the convolutional operator is that the latent concepts are linearly separable.&lt;/strong&gt; However, the conventional linear filters impose a heavyburden on the neural network.&lt;/p&gt;

&lt;h4 id=&quot;32-why-is-the-micro-network-used&quot;&gt;3.2 why is the micro-network used?&lt;/h4&gt;

&lt;p&gt;what we hope? &lt;strong&gt;Given no priors about the distributions of the latent concepts, it is desirable to use a universal func- tion approximator for feature extraction of the local patches, as it is capable of approximating more abstract representations of the latent concepts.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The “micro-network” is considered as a more general linear model which is capable of approxmating any convex functions. (which is more looser than the conventional neural network). Why does MLP used? 1. MLP is compatible wirh the strure of the convolutional neural work, which is trained using back-propagation. 2. MLP  can be a deep model itself, which is consistent with the spirit of feature re-use.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/9geikgfsr9xw7swg5ab35icq/image_1bghst2reprtdt12detc1ge89.png&quot; alt=&quot;image_1bghst2reprtdt12detc1ge89.png-14.8kB&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here $n$ is the number of layers in the multilayer perceptron. Rectified linear unit is used as the activation function in the multilayer perceptron.&lt;/p&gt;

&lt;p&gt;From cross channel(cross featiure map) pooling point of view, this equation is equivalent to cascade cross channel parametric pooling on a normal convolutional layer. For example, 1x1 convolutional kernel is a linear combinations of the cross channels.&lt;/p&gt;

&lt;h4 id=&quot;33-whats-the-global-average-pooling-operator&quot;&gt;3.3 what’s the global average pooling operator&lt;/h4&gt;

&lt;p&gt;Global average pooling operator is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer. This designed structure has no parameters to optimize thus overfitting is avoided at this layer.&lt;/p&gt;

&lt;h4 id=&quot;34-the-overall-structure-of-network-in-network&quot;&gt;3.4 the overall structure of network in network&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;http://static.zybuluo.com/GwanSiu/bytj7t6iqq8iivkqxgi2go4g/image_1bghtln474gq52r19gk84v1b029.png&quot; alt=&quot;image_1bghtln474gq52r19gk84v1b029.png-76.5kB&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 20 May 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/05/20/network-in-network/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/05/20/network-in-network/</guid>
        
        <category>Machine Learning</category>
        
        
      </item>
    
      <item>
        <title>《流血的仕途》</title>
        <description>&lt;h1 id=&quot;1书评&quot;&gt;1.书评&lt;/h1&gt;

&lt;p&gt;断断续续用了十多天读完的了《流血的仕途》，在那个战火纷飞，人才辈出的年代，百家争鸣，思想鼎盛，是个最好的时代，但国家之间的尔虞我诈，合纵连横，也是个最坏的时代。本书以李斯的一生仕途为线索，见证了大秦帝国的兴衰。对于李斯，我的评价是：成于李斯，败也李斯。大秦帝国的崩塌，秦二世胡亥信小人，重赋税，杀忠臣，坑良将，本是王国之君的节奏，民众造反也在情理之中（自古以来都是官逼民反）。但要究其缘由，嬴政和李斯作为帝国的创造者，也有脱不开的关系，间接造成大清帝国的倒塌。&lt;/p&gt;

&lt;p&gt;回到对本书的评价中，《流血的仕途》收录了多篇名篇，如：李斯的谏逐客书，韩非的韩非子。但是作者通篇照搬，却不对此作出应有的解释，实在让人难以读懂。对此，我觉得作者有卖弄的嫌疑。另外，作者在全书中大开上帝视角，博古通今，学贯中西，在对事件描述的时候，还要穿插着自己对心理学的理解，让人读起来十分别扭，显得不伦不类, 史不成史。最后，作者在对心理学的描述中，许多白话部分甚是意淫之作，并且明显不符合人物所在的历史情境之中。&lt;/p&gt;

&lt;p&gt;对本书的总结：此书虽以小说的形式书写大秦帝国的兴衰，但却难以与《明朝那些事》媲美，不够严谨，有卖弄学问，堆砌文献的嫌疑，显得不伦不类。&lt;/p&gt;

&lt;h1 id=&quot;2-李斯为什么会成功&quot;&gt;2. 李斯为什么会成功&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;教育背景：李斯师从荀子，与韩非同窗。荀子门下，谈笑有鸿儒，往来无白丁。李斯五年学习，其眼界和格局，与非上蔡一乡官可比的。另外，在学习期间，与韩非结交。三人行，必有我师焉，韩非使李斯进一步成长，为李斯日后的丞相之路打下了扎实的基础。&lt;/li&gt;
  &lt;li&gt;李斯对时局的分析非常准确。李斯厉害之处，便是会把握住时机给自己创造出让自己进阶的机会。每一次机会对李斯是一场命运的赌博，李斯输不起的赌博，李斯的果断，勇气让人钦佩。首先依据秦国情势，选择抱住丞相吕不韦的大腿，在众多舍人中脱颖而出。所谓乱世出英雄，李斯利用嫪毐，造成秦国政治格局清洗。李斯一直明白自己需要什么，要做什么，果断站在嬴政这边。（李斯的投资无疑是最正确的，站在未来的潜力无穷的王者队伍里），不受嫪毐的招贤，亦不与吕不韦走得太近。这是一种功力，在水火不容的三者之间游走。作为谋臣，知嬴政之所知，想嬴政之所想，懂的收敛分寸，不功高震主，也不居功自傲（在做特务科科长时，本本份份做事，懂的隐忍）&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;3-李斯为什么会失败&quot;&gt;3. 李斯为什么会失败&lt;/h1&gt;

&lt;p&gt;李斯的失败用一句话可以总结：贪恋虚无缥缈的身后事，一步错则步步皆输。最关键的一步：与赵高合谋，选择一个为己私利而不择手段，毫无底线的队友，也同时授人以柄，拥护胡亥昏君当道。当赵高迫害蒙氏兄弟，迫害冯氏丞相的时候，削其羽翼，他没说话，最后当他自己被赵高迫害时，已经没有人替他说话了。李斯本有翻盘的机会，若与赵高鱼死网破，拥子婴为帝，秦王朝还有一线生机。李斯聪明一世，早期识得嫪毐非同道中人，年老了却认不清赵高的为人，本性难移。赵高，小人也，不足与之为谋。&lt;/p&gt;

&lt;h1 id=&quot;4-覆灭的大秦帝国&quot;&gt;4. 覆灭的大秦帝国&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;直接原因：秦二世的荒淫无道，无道指的是不使仁政。为什么？帝国结束完春秋战国时的战乱，又加上秦始皇时期重刑法，重赋税，本应仁政抚恤，韬光养晦，增强国力，但秦二世的做法无异于火上浇油.&lt;/li&gt;
  &lt;li&gt;秦始皇对死亡的恐惧，对不死的执着，使他一直迟迟不立太子，为后面的动荡不安埋下了种子。其次，对赵高的过度信任，让赵高任中车府令，皇帝的秘书长，这是用人的一大失误，最后使得遗诏任人篡改.&lt;/li&gt;
  &lt;li&gt;李斯与赵高二世结盟，坑爹铁三角坑完了整个大秦帝国。&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 13 May 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2017/05/13/%E6%B5%81%E8%A1%80%E7%9A%84%E4%BB%95%E9%80%94/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/05/13/%E6%B5%81%E8%A1%80%E7%9A%84%E4%BB%95%E9%80%94/</guid>
        
        <category>Reading</category>
        
        
      </item>
    
      <item>
        <title>Hello 2015</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;“Yeah It’s on. ”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;Hux 的 Blog 就这么开通了。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#build&quot;&gt;跳过废话，直接看技术实现 &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2015 年，Hux 总算有个地方可以好好写点东西了。&lt;/p&gt;

&lt;p&gt;作为一个程序员， Blog 这种轮子要是挂在大众博客程序上就太没意思了。一是觉得大部分 Blog 服务都太丑，二是觉得不能随便定制不好玩。之前因为太懒没有折腾，结果就一直连个写 Blog 的地儿都没有。&lt;/p&gt;

&lt;p&gt;在玩了一段时间知乎之后，答题的快感又激起了我开博客的冲动。之前的&lt;a href=&quot;http://huangxuan.me/portfolio&quot;&gt;个人网站&lt;/a&gt;是作品集形式的（现在集成进来了），并不适合用来写博文，一不做二不休，花一天搞一个吧！&lt;/p&gt;

&lt;p id=&quot;build&quot;&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;正文&quot;&gt;正文&lt;/h2&gt;

&lt;p&gt;接下来说说搭建这个博客的技术细节。&lt;/p&gt;

&lt;p&gt;正好之前就有关注过 &lt;a href=&quot;https://pages.github.com/&quot;&gt;GitHub Pages&lt;/a&gt; + &lt;a href=&quot;http://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; 快速 Building Blog 的技术方案，非常轻松时尚。&lt;/p&gt;

&lt;p&gt;其优点非常明显：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Markdown&lt;/strong&gt; 带来的优雅写作体验&lt;/li&gt;
  &lt;li&gt;非常熟悉的 Git workflow ，&lt;strong&gt;Git Commit 即 Blog Post&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;利用 GitHub Pages 的域名和免费无限空间，不用自己折腾主机
    &lt;ul&gt;
      &lt;li&gt;如果需要自定义域名，也只需要简单改改 DNS 加个 CNAME 就好了&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Jekyll 的自定制非常容易，基本就是个模版引擎&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本来觉得最大的缺点可能是 GitHub 在国内访问起来太慢，所以第二天一起床就到 GitCafe(Chinese GitHub Copy，现在被 Coding 收购了) 迁移了一个&lt;a href=&quot;http://huxpro.coding.me&quot;&gt;镜像&lt;/a&gt;出来，结果还是巨慢。&lt;/p&gt;

&lt;p&gt;哥哥可是个前端好嘛！ 果断开 Chrome DevTool 查了下网络请求，原来是 &lt;strong&gt;pending 在了 Google Fonts&lt;/strong&gt; 上，页面渲染一直被阻塞到请求超时为止，难怪这么慢。&lt;br /&gt;
忍痛割爱，只好把 Web Fonts 去了（反正超时看到的也只能是 fallback ），果然一下就正常了，而且 GitHub 和 GitCafe 对比并没有感受到明显的速度差异，虽然 github 的 ping 值明显要高一些，达到了 300ms，于是用 DNSPOD 优化了一下速度。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;配置的过程中也没遇到什么坑，基本就是 Git 的流程，相当顺手&lt;/p&gt;

&lt;p&gt;大的 Jekyll 主题上直接 fork 了 Clean Blog（这个主题也相当有名，就不多赘述了。唯一的缺点大概就是没有标签支持，于是我给它补上了。）&lt;/p&gt;

&lt;p&gt;本地调试环境需要 &lt;code class=&quot;highlighter-rouge&quot;&gt;gem install jekyll&lt;/code&gt;，结果 rubygem 的源居然被墙了……后来手动改成了我大淘宝的镜像源才成功&lt;/p&gt;

&lt;p&gt;Theme 的 CSS 是基于 Bootstrap 定制的，看得不爽的地方直接在 Less 里改就好了（平时更习惯 SCSS 些），&lt;strong&gt;不过其实我一直觉得 Bootstrap 在移动端的体验做得相当一般，比我在淘宝参与的团队 CSS 框架差多了……&lt;/strong&gt;所以为了体验，也补了不少 CSS 进去&lt;/p&gt;

&lt;p&gt;最后就进入了耗时反而最长的&lt;strong&gt;做图、写字&lt;/strong&gt;阶段，也算是进入了&lt;strong&gt;写博客&lt;/strong&gt;的正轨，因为是类似 Hack Day 的方式去搭这个站的，所以折腾折腾着大半夜就过去了。&lt;/p&gt;

&lt;p&gt;第二天考虑中文字体的渲染，fork 了 &lt;a href=&quot;http://www.typeisbeautiful.com/&quot;&gt;Type is Beautiful&lt;/a&gt; 的 &lt;code class=&quot;highlighter-rouge&quot;&gt;font&lt;/code&gt; CSS，调整了字号，适配了 Win 的渣渲染，中英文混排效果好多了。&lt;/p&gt;

&lt;h2 id=&quot;后记&quot;&gt;后记&lt;/h2&gt;

&lt;p&gt;回顾这个博客的诞生，纯粹是出于个人兴趣。在知乎相关问题上回答并获得一定的 star 后，我决定把这个博客主题当作一个小小的开源项目来维护。&lt;/p&gt;

&lt;p&gt;在经历 v1.0 - v1.5 的蜕变后，这个博客主题愈发完整，不但增加了诸多 UI 层的优化（opinionated）；在代码层面，更加丰富的配置项也使得这个主题拥有了更好的灵活性与可拓展性。而作为一个开源项目，我也积极的为其完善文档与解决 issue。&lt;/p&gt;

&lt;p&gt;如果你恰好逛到了这里，希望你也能喜欢这个博客主题。&lt;/p&gt;

&lt;p&gt;—— Hux 后记于 2015.10&lt;/p&gt;

</description>
        <pubDate>Thu, 29 Jan 2015 20:00:00 +0800</pubDate>
        <link>http://localhost:4000/2015/01/29/hello-2015/</link>
        <guid isPermaLink="true">http://localhost:4000/2015/01/29/hello-2015/</guid>
        
        <category>生活</category>
        
        
      </item>
    
  </channel>
</rss>
