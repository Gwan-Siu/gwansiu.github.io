---
layout: post
title: Markov Chain Monte Carol (MCMC)
date: 2018-04-05
author: Gwan Siu
catalog: True
tags:
    - Machine Learning
---

## 1. Monte Carlo methods 

### 1.1 Monte Carlo methods are algorithm that:

- Generate samples from a given probability distribution $p(x)$.
- Estimate expectations of functions $\mathbb{E}[f(x)]$ under a distribution $p(x)$

### 1.2 Why Monte Carlo useful?

- Can use samples of $p(x)$ to approximate $p(x)$ itself, allow us to do graphical model inference when we can't compute $p(x)$.

- Expectation $\mathbb{E}[f(x)]$ reveal interesting properties about $p(x)$, e.g. means and variances.

### 1.3 Limitation of Monte Carlo

- Direct sampling
  - Hard to get rare events in high-dimensional spaces.
  - Infesible for MRFs, unless we know the normalizer $Z$.

- Rejection sampling, Important sampling
  - Do not work well if the proposal $Q(x)$ is very different from $P(x)$.
  - Construct a $Q(x)$ similar to $P(x)$ can be diffuclt
    - Requires knowledge of analytical form of $P(x)$ - but if we had that, we wouldn't even need to sample

- Intuition:Instead of a fixed proposal $Q(x)$, use a adaptive proposal.


## 2. Markov Chain Monte Carol(MCMC)

MCMC algorithms provides adaptive proposals probability.

- Instead of $Q(x^{\prime})$, use $Q(x^{\prime}\vert x)$ where $x^{\prime}$ is the new state being sampled, and $x$ is the previous sample.

- As $x$ changes, $Q(x^{\prime}\vert x) can also change (as a function of $x^{\prime}$).

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/other/MCMC.png" width = "600" height = "400"/>


### 2.1 What's the Markov Chain

**([Definition](https://en.wikipedia.org/wiki/Markov_chain):)** A Markov Chain is a stochastic process that a sequence of random variable $x_{1},x_{2},...,x_{n}$ with markov property

$$
\begin{equation}
T(x_{n+1}=x\vert x_{1},x_{2},...,x_{n})=T(x_{n+1}=x\vert x_{n})
\end{equation}
$$

the probability of the next step only depends on the current state. 

- Random variable $x_{i}$ can be vectors. We define $x_{i}$ to be the $t-$th sample of all variables in a graphical model.
- $x_{i}$ represents the entire state of the graphical model at time $t$.

We study homogenous Markov Chains, in which the transition kernel $P(x_{n}=x\vert x_{n-1})$ is fixed with time. To emphasize this, we will call the kernel $T(x^{\prime}\vert x)$, where $x$ is the previous state and $x^{\prime}$ is the next state.


### 4.2 Some Markov Chains

**Homogenous Markov Chain:** A chain is homogeneous at step $t$ if the transition probabilities are independent of tt. Thus the evolution of the Markov chain only depends on the previous state with a fixed transition matrix.

**Irreducible Markov Chain:** Every state is accessible in a finite number of steps from another state. That is, there are no absorbing states. In other words, one eventually gets everywhere in the chain.(*example:* Consider as an example surfing the web. We do want to reach all parts of the web so we dont want to be trapped into an subset.)

**Recurrent:** States visited repeatedly are recurrent: positive recurrent if time-to-return is bounded and null recurrent otherwise. Harris recurrent if all states are visited infinitely often as $t\rightarrow \infty$.

**Aperiodic:** There are no deterministic loops. This would be bad in our web example as well as we would be stuck in a loop at some pages.

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/other/0D4DDB79-114A-4DBB-BDC4-A6E16601B700.png" width = "600" height = "400"/>

### 4.3 Stationary Markov Chain

**(Formal Definition of Stationarity):** A staionary Markov Chain produces the same marginal distribution when multiplied by the transition matrix. That is 

$$
\begin{equation}
sT = s \text{ or }\sum_{i}s_{i}T_{ij}=s_{j}
\end{equation}
$$

In the case of continuous state space, which are the ones we encounter in sampling, if the transition kernel T is defined so that

$$
\begin{equation}
\int dx_{i}s(x_{i})T(x_{i+1}\vert x_{i})=s(x_{i+1})
\end{equation}
$$

then 

$$
\begin{equation}
\int dxs(x)T(y\vert x)=s(y)
\end{equation}
$$

**Ergodicity**: Aperiodic, irreducible, positive Harris recurrent markov chains are ergodic, that is, in the limit of infinite (many) steps, the marginal distribution of the chain is the same(the probability of all state remain stable, nonzero and is independent with the initial position). This means that if we take largely spaced about samples from a stationary markov chain, we can draw independent samples.

$$
\begin{equation}
\int g(x)f(x)dx=\frac{1}{N}\sum_{j=B+1}^{B+N}g(x_{j})
\end{equation}
$$

Here B is called the burin (which comes from the approach to stationarity after a while) and T is called the thinning (which comes from ergodicity). So we have this “ergodic” law of large numbers.

([摘抄慕课网笔记](https://mooc.guokr.com/note/15627/))在满足一定条件的情况下，马尔可夫过程将收敛至一个均衡。这是一个统计均衡，在每种状态下的概率是固定不变的，但事物将依旧在各个状态间转移。

马尔可夫过程收敛到均衡的四个条件：

一、可能的状态数量是有限的。

二、转移概率固定不变。

三、从任意一个状态能够变到任意其他一个状态。有可能不是从状态A直接变到状态C，而是先变到状态B再变到C，但只要有路径从状态A变成状态C就行。

四、过程不是简单循环。比如不能是从全A变到全B，然后又自动从全B变到全A。

马尔可夫收敛定理（Markov Convergence Theorem）：如果满足上述四个条件，一个马尔科夫过程将收敛到一个均衡状态，且此均衡唯一。

**只要转移概率不变，那么初始状态、历史过程、中途干预都不重要，最后必将达到那个唯一的均衡。换句话说，马尔科夫链最后达到的均衡与初始状态，转移过程以及中途干预无关。** 

```python

import scipy.stats as st

def target(lik, prior, n, h, theta):
    if theta < 0 or theta > 1:
        return 0
    else:
        return lik(n, theta).pmf(h)*prior.pdf(theta)

def mh_coin(niters, n, h, theta, lik, prior, sigma):
    samples = [theta]
    while len(samples) < niters:
        theta_p = theta + st.norm(0, sigma).rvs()
        rho = min(1, target(lik, prior, n, h, theta_p)/target(lik, prior, n, h, theta ))
        u = np.random.uniform()
        if u < rho:
            theta = theta_p
        samples.append(theta)
    return samples

n = 100
h = 61
lik = st.binom
prior = st.beta(10, 10)
sigma = 0.05
niters = 100

sampless = [mh_coin(niters, n, h, theta, lik, prior, sigma) for theta in np.arange(0.1, 1, 0.2)]

# Convergence of multiple chains

for samples in sampless:
    plt.plot(samples, '-o')
plt.xlim([0, niters])
plt.ylim([0, 1]);
```

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/Monte%20Carol%20Method/image/mcmc-recure.png" width = "600" height = "400"/>

### 4.3 Markov Chain and Monte Carol Methods

A irreducible (goes everywhere) and aperiodic (no cycles) markov chain will converge to a stationary markov chain. It is the marginal distribution of this chain that we want to sample from, and which we do in metropolis (and for that matter, in simulated annealing).

As we can see above, to find stationary distribution, we need to solve an eigenvector proble. This can be hard.

However, A sufficient, but not necessary, condition to ensure that s(x)s(x) is the desired stationary distribution is the already seen reversibility condition, also known as detailed balance:

$$
\begin{equation}
\int dxs(x)t(y\vert x)=s(y)\int dxT(x\vert y)
\end{equation}
$$

which gives back us back the stationarity condition from above.

Thus we want to design us samplers which satisfy detail balance.

### 4.4 Metropolis Hasting Algorithm(MH Algorithm)

**(why do we need Metropolis Hasting Algorithm?)** We've learnt how to do the inverse transform and how to use rejection sampling with a majority function. So why do we use these methods to sample a ditribution? ** inefficient as dimensions increased.** In other words, dimension curse. **How do we understand this point?**

In generally, we want to calculate the expectation of distribution as sample average, however, as dimension of space increased, majorizing in multiple dimensions can have us spending a lot of time in tail dimension because you leave more and more space out.  If inverse tranform and reject sampling methods are adopted, then it will boost inefficient.

In multiple dimensions, volumns get smaller and smaller, that's the curse of dimension. This concept can be shown as:

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/other/D5B3AE61-1E06-4445-AE57-9263315A536E.png" width = "600" height = "400" alt="Important Sampling"/>

where the centre-partitions combination to an integral goes from 1/3rd to 1/27th. Now suppose the mode of the distibution is contained in this partition: then its contribution to the integral is going down with dimensions.

As the centre volume decreases, the outer volume increases, but this is in distribution tails, so we dont get much of a contribution from there either:

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/other/259532EC-97B2-4409-99FD-C2293E93D528.png" width = "600" height = "400" alt="Important Sampling"/>

It is the neighborhood between these extremes, called the typical set which our sampler must explore well. And to get a good rejection sampling majorizer for this becomes hard.

**The idea of MH Algorithm**

1. Use a proposal distribution to propose s step
2. Then we calculate the pdf at that step, and compare it to the one at that previous step.
3. If the probability increased we accept. If the probability decreased, we accept the some of time, based on the ratio of the new probability to the old one.
4. We accumulate our samplees, as we now trying to sample a distribution.

**MH Algorithm**

1. initialize $x^{(0)}$.
2. Draw $\mu \sim U(0,1)$.
3. Draw propose $x^{(\ast)} \sim q(x^{(\ast)}\vert x)$.
4. If $\mu<\min(1, \frac{s(x^{(\ast)})q(x\vert x^{(\ast)})}{s(x)q(x^{(\ast)}\vert x)})$, $x^{(i+1)}=x^{(\ast)}$. Else, $x^{(i+1)}=x$
5. back step 2 for loop.

**Why does MH algorithm work?**

To prove if MH algorithm work is to prove MH algorithm satisfied detail balance condition.

Transition Kernel $K(x\rightarrow x^{\ast})$ includes the joint density of the following:

  1. Propose $x^{(\ast)}$ from the $q(x^{(\ast)} \vert x)$.
  2. the accept $x^{(\ast)}$ with the ratio $\alpha(x^{(\ast)}, x)=\min(1,\frac{s(x^{(\ast)})q(x\vert x^{(\ast)})}{s(x)q(x^{(\ast)}\vert x)})$

To verify the detail balance condition:

$$
\begin{aligned}
s(x^{*})q(x^{*}\vert x)\alpha(x^{*},x) &= s(x^{*})q(x^{*}\vert x)\min(1,\frac{s(x^{*})q(x\vert x^{*})}{s(x)q(x^{*}\vert x)}) \\
&= \min(s(x)q(x^{*}\vert x), s(x^{*})q(x\vert x^{*})) \\
&= s(x)q(x\vert x^{*})\min(1,\frac{s(x)q(x^{*}\vert x)}{s(x^{*})q(x\vert x^{*})}) \\
&= s(x)q(x\vert x^{*})\alpha(x,x^{*})
\end{aligned}
$$

**Code of Metropolis Hasting Algorithm**

```python

def metropolis_hastings(p,q, qdraw, nsamp, xinit):
    samples=np.empty(nsamp)
    x_prev = xinit
    accepted=0
    for i in range(nsamp):
        x_star = qdraw(x_prev)
        p_star = p(x_star)
        p_prev = p(x_prev)
        pdfratio = p_star/p_prev
        proposalratio = q(x_prev, x_star)/q(x_star, x_prev)
        if np.random.uniform() < min(1, pdfratio*proposalratio):
            samples[i] = x_star
            x_prev = x_star
            accepted +=1
        else:#we always get a sample
            samples[i]= x_prev
            
    return samples, accepted

# target function
f = lambda x: 0.554*x*np.exp(-(x/1.9)**2)

x = np.linspace(0,10,100)
plt.plot(x, f(x), 'r')
plt.grid('on')
plt.title('The target function')

```

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/Monte%20Carol%20Method/image/mh.png" width = "600" height = "400"/>

```python
from scipy.stats import gamma

t=10.0

def gammapdf(x_new, x_old):
    return gamma.pdf(x_new, x_old*t, scale=1/t)

def gammadraw(x_old):
    return gamma.rvs(x_old*t,scale=1/t)

x_init = np.random.uniform()
samps, acc = metropolis_hastings(f, gammapdf, gammadraw, 100000, x_init)

# plot our sample histogram
plt.hist(samps,bins=100, alpha=0.4, label=u'MCMC distribution', normed=True) 
somesamps=samps[0::20000]
for i,s in enumerate(somesamps):
    xs=np.linspace(s-3, s+3, 100)
    plt.plot(xs, gamma.pdf(xs,s*t,scale=1/t),'k', lw=1)
xx= np.linspace(0,10,100)
plt.plot(xx, f(xx), 'r', label=u'True distribution') 
plt.legend()
plt.xlim([0,10])
plt.show()
print("starting point was ", x_init)

```

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/Monte%20Carol%20Method/image/mh-1.png" width = "600" height = "400"/>

## 5. Gibbs Sampling

### 5.1 The idea of Gibbs Sampling

The idea of gibbs as a markov chain in which the transition matrix can be obtained as the kernel of the an integral fixed point equation by sampling alternatively from two conditionals.

Gibbs determined the energy states of gases at equilibrium by cycling through all the particles, drawing from each one of them conditionally given the enerygy levels of the others, taking the time average. 

Now, suppose you have a density function of two variables $(x,y)$. You wish to sample from this density.

The definition of the X marginal is 

$$
\begin{equation}
f_{X}(x)=\int f_{XY}(x,y)dy
\end{equation}
$$

We reformulate the above formula:

$$
\begin{equation}
f_{X}(x)=\int f_{XY}(x,y)dy=\int f(x\vert y)f(y)dy=\int f(x\vert y)dy\int f(y\vert x^{'})f(x^{'})dx^{'}
\end{equation}
$$

Thus

$$
\begin{equation}
f(x)=\int h(x,x^{'})f(x^{'})dx^{'}
\end{equation}
$$

where

$$
\begin{equation}
h(x,x^{'})=\int f(x\vert y)f(y\vert x^{'})dy
\end{equation}
$$

Now consider an iterative scheme in which the “transition kernel” $h(x,x′)$ is used to create a proposal for metropolis-hastings moves. This looks like:

$$
\begin{equation}
f(x_{t})=\int h(x_{t},x_{t-1})f(x_{t-1})dx_{t-1}
\end{equation}
$$

which is the equation of stationary distribution.

The big idea, then, here, as in the case of markov chains, is that the above equation can be thought of as a fixed-point integral equation, and thus we can think of an iterative process which at first does not satisy this condition but then does as time goes on and we reach stationarity.

Similarly here, if we draw $y$, from the conditional $f(y\vert x)$ and then $x$ again from $f(x\vert y)$ we will be able to get the marginal distribution of $x$. Symmetrically we can get the marginal for $y$.

Now, if I can draw from the $x$ marginal, and the $y\vert x$ conditional, i can draw from the $x,y$ joint, and I am done.

### 5.2 Gibbs Sampling Algorithm

- Given a string sampling $(x_{1},y_{1},z_{1})^{T}$.
- You want to sample: ${(x_{2},y_{2},z_{2})^{T}, (x_{3},y_{3},z_{3})^{T},...,(x_{N},y_{N},z_{N})^{T}}\sim P(x,y,z)$.
- Then the algorithm goes:

$$
\begin{aligned}
x_{2}&\sim P(x\vert y_{1},z_{1}) \\
y_{2}&\sim P(y\vert x_{2},z_{1}) \\
z_{2}&\sim P(z\vert y_{2},x_{2}) \\
&... \\
x_{3}&\sim P(x\vert y_{2},z_{2}) \\
y_{3}&\sim P(y\vert x_{3},z_{2}) \\
z_{3}&\sim P(z\vert x_{3},y_{3}) \\
\end{aligned}
$$

### 5.3 Gibbs and MH Algorithm

Gibbs is the extension of MH Algorithm in high dimension and also is a special case of MH Algorithm without rejection. I will show you the proof.

Look at the M-H acceptance ratio:

- Let **x**=$x_{1},...,x_{D}$.
- When sampling $k$th component, $q_{k}(x^{*}\vert x)=\pi(x_{k}^{*}\vert x_{-k})$.
- When sampling $k$th component, $x^{*}_{-k}=x_{-k}$.

$$
\begin{equation}
\frac{\pi(x^{*})q(x\vert x^{*})}{\pi(x)q(x^{*}\vert x)}= \frac{\pi(x^{*})q_{k}(x\vert x^{*}_{-k})}{\pi(x)q(x^{*}_{k}\vert x_{-k})} = \frac{\pi(x^{*}\vert x^{*}_{-k})q_{k}(x\vert x^{*}_{-k})}{\pi(x_{k}\vert x_{-k})q(x^{*}_{k}\vert x_{-k})}=1
\end{equation}
$$

**Code of Gibbs sampling**

$$
\begin{equation}
f(x,y)=x^{2}\text{exp}(-xy^{2}-y^{2}-2*y-4*x)
\end{equation}
$$

```python

func = lambda x,y: x**2*np.exp( -x*y**2 - y**2 + 2*y - 4*x )

numgridpoints=400
x = np.linspace(0,2,numgridpoints)
y = np.linspace(-1,2.5,numgridpoints)
xx,yy = np.meshgrid(x,y)
zz = np.zeros((numgridpoints,numgridpoints))
for i in np.arange(0,numgridpoints):
    for j in np.arange(0,numgridpoints):
        zz[i,j]=func(xx[i,j],yy[i,j])
        
plt.contourf(xx,yy,zz)

```
<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/Monte%20Carol%20Method/image/gibbs.png" width = "600" height = "400"/>