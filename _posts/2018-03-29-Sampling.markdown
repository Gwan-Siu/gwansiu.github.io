---
layout: post
title: Sampling Methods
date: 2018-03-29
author: Gwan Siu
catalog: True
tags:
    - Machine Learning
---

## 1.Introduction

>In this article, I will discuss some sampling methods: **Inverse Samling**, **Reject-Accept Sampling**, **Important Sampling**, and its application partical filter.

**Why do we need sampling?**  In many inference tasks (such as computing marginal probability $P(x)$, computing the partition function $A(\theta)$, or find the expectation of arbitrary function), we are interested in quatities that in a sense sum over the configuration of a true distribution

$$
\begin{equation}
E_{p}(f(x)) = \int f(x)p(x)\mathrm{d}x
\end{equation}
$$

In such problems, true distribution in closed-form is hard to be obtained and this integral operator in high-dimensional space is intractable. However, if we can sample from this distribution, approximate inference is possible by using a sample-based method of $p(x)$ because large number theory. Here, we have $N$ sample from the true distribution and the expectation of this true distribution can be approximated by

$$
\begin{equation}
E_{p}(f(x))=\frac{1}{N}\sum_{n}f(x_{n})
\end{equation}
$$

this is essentail idea behind of the Monte Carol, which gives us a stochastic representation of a potentially complex function. This representation can then be used to compute the marginals and expectations in which we are interested. Actually, the approximate are asymptotically exact (they get close to the true $E_{p}[f(x)]$ as $N\rightarrow \infty$), and flexible for any distribution. However, there are key non-trival challenges that must be overcome:

1. How exactly do we draw from them from complex distributions?
2. Not all samples are equally useful.
3. How do we knwo we've draw enough samples.


## 2. Inverse sampling (naive)

The idea behind inverse sampling is very ituitive: to transform uniform samples into samples from a different distribution. That is, by somehow drawing from an uniform distribution(because CDF is range from 0 to 1), we make it possible to draw from the other distribution. The procedure of inverse sampling is illustrated as:

<img src="http://www.howardrudd.net/wp-content/uploads/2015/02/InverseTransform23.png" width = "600" height = "400"/>

The assumption of inverse sampling is that **CDF must be invertiable!** 

The algorithm of inverse sampling is:
    
1. get a uniform sample $\mu$ from **U** (0,1)
2. obtain the sample $x$ through $x=F^{-1}(\mu)$ where F is the CDF distribution we desire.
3. repeat.

**Why does inverse sampling work?**

To be note that:

$$
            \begin{equation}
            F^{-1}(\mu)= \text{ smallest }x \text{ such that } F(x)\geq \mu
            \end{equation}
$$

What's the ditribution does the random variable $y=F^{-1}(\mu)$ follow?

The CDF of y is $p(y\leq x)$. Since the CDF is monotonic, we can obtain without loss of generity:

$$
    \begin{equation}
    p(y\leq x)=p(F(y)\leq F(x))=p(\mu\leq F(x))=F(x)
    \end{equation}
$$

Thus we get the CDF and hence the pdf from which we want to sample.

**Limitation of Inverse Sampling**

Not all the pdf has analytical CDF, for example, gaussian ditribution. In addition, for some complex distributions, the inverse CDF may be complicated and it is hard to do inverse sampling.

## 2. Rejection Sampling

### 2.1 Basic Rejection Sampling

The basic idea is come up with von Neumann. **If you have a complex function $p(x)$ you are trying to sample from, whose maximum value and minimum value are knowbm, basically accept the sample by generating a uniform random number at any $x$ and the range of $p(x)$, and accepting it if the value is below the value of the function at that $x$.** The procedure of basic sampling is illustrated as:


<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/other/00A4D982-E912-43B3-A2B6-9B3DCA126A97.png" width = "600" height = "400"/>


The procedure of basic rejection sampling:

1. Draw $x$ uniformly from $[x_{min}, x_{max}]$.
2. Draw $y$ uniformly from $[0, y_{max}]$.
3. If $y\leq f(x)$, accept. Else, reject.
4. Repeat.

**The intuitive explaination:** This works as more samples will be accepted in the regions of $x$-space where the function $f$ is higher: indeed they will be accepted in the ratio of the height of the function at any given $x$ to $y_{max}$.  **From the perspective of probability interpretation,** the accept-to-total ratio reflects the probability mass in each x silver. 

**Code of Basic Rejection Sampling**

```python
## target function

f = lambda x: np.exp(-x)
#f = lambda x x**2

## domain limits
xmin = 0 # lower bound of feasiable domain
xmax = 10 #upper bound of feasiable domain

## range limit for y
ymax=1
#ymax = 100

N = 10000 #the total of samples
accept =0 #count the total number of acceptance
samples = np.zeros(N)
count = 0

while (accept <N):
    
    x = np.random.uniform(xmin,xmax)
    y = np.random.uniform(0, ymax)
    
    if y<f(x):
        samples[accept]=x
        accept += 1
    count +=1

print('Count: ', count, ', Accepted: ', accept)

hinfo = np.histogram(samples, 30)
plt.hist(samples, bins=30, label='Smaples')
print(hinfo[0][0])
xvals = np.linspace(xmin, xmax, 10000)
plt.plot(xvals, hinfo[0][0]*f(xvals), 'r', label='f(x)')
plt.grid('on')
plt.legend()

```

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/Monte%20Carol%20Method/image/rejection-sampling.png" width = "600" height = "400"/>

### 2.2 Modified Reject Sampling

Low accetance rate is one deterministic limitation of basic rejectioin sampling. Now let's consider a case where

- The **target distribution** $p(x)$ is difficult to sample from.
- The **unnormalized distribution** $\hat{p}(x)=\frac{1}{Z}p(x)$ is easy to evaluate. Note that this alone does not make $p(x)$ amensable to sampling.
- The **proposal distribution** $q(x)$ is a distribution that we can easily sample from (e.g. uniform or normal)
- $k$ is chosen constant such that $kq(x)\geq \hat{p}(x)$ for all $x$. This is called the comparison function.

**Procedure**

1. Sample $x_{0}$ from $q(x)$.
2. Sample a number $\mu_{0}$ from the uniform distribution over $[0, kq(x_{0})]$
3. Reject the sample if $\mu_{0}>\hat{p}(x_{0})$ and retain the sample otherwise.

Note that the probability of accepting sample $x_{0}$ is $\frac{\hat{p}(x_{0})}{kq(x_{0})}$. Pictorially for a univariate case, this process is akin to sampling uniformly any point in the area under the $kq(x)$ curve and accepting only if it does not land in the gray region.

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/other/reject_sample.png" width = "600" height = "400"/>


**Correctness**

Formally show that this procedure samples correctly from $p(x)$. Firstm we observe that the procedure selects a particular $x$ with density proportional to $q(x)\cdot \frac{\hat{p}(x)}{kq(x)}$. Then, the sampling mechanism generates samples according to  a distribution $p_{s}(x)$ which is equal to

$$
\begin{align}
p_{s}(x) &= \frac{q(x)\frac{\hat{p}(x)}{kq(x)}}{\int q(x)\frac{\hat{p}(x)}{kq(x)}\mathrm{d}x} \\
&=\frac{\hat{p}(x)}{\int \hat{p}(x)\mathrm{d}x} \\
&=p(x)
\end{align}
$$

**Limitation**

If the proposal distribution $q(x)$ is not chosen well (i.e. differs greatly from $p(x)$), then even an optimally chosen $k$ can result tin a huge rejection region. This implies a large waste of samples that will be rejected. Even if distributions seem similar, in high dimensions this rejection volumn can be very large. For example, $d-$dimensional gaussians

$$
\begin{align}
Q&\sim N(\mu, \sigma_{q}^{2/d}) \\
P&\sim N(\mu, \sigma_{p}^{2/d})
\end{align}
$$

for $d=1000$ and $\sigma_{q}$ only 1 percent bigger than $\sigma_{p}$ result in an acceptance rate of only $\approx \frac{1}{2000}$. One potential way to fix this is to use adaptive rejection sampling, which covers $\tilde{p}$ with an envelope of piecewise functions instead of one proposal distribution $q$ but this gets rather complicated.

**The code of modified rejection sampling**

```python

p = lambda x: np.exp(-x)  # our distribution
g = lambda x: 1/(x+1)  # our proposal pdf (we're thus choosing M to be 1)
invCDFg = lambda x: np.log(x +1) # generates our proposal using inverse sampling

# domain limits
xmin = 0 # the lower limit of our domain
xmax = 10 # the upper limit of our domain

# range limits for inverse sampling
umin = invCDFg(xmin)
umax = invCDFg(xmax)

N = 10000 # the total of samples we wish to generate
accepted = 0 # the number of accepted samples
samples = np.zeros(N)
count = 0 # the total count of proposals

# generation loop
while (accepted < N):
    
    # Sample from g using inverse sampling
    u = np.random.uniform(umin, umax)
    xproposal = np.exp(u) - 1
    
    # pick a uniform number on [0, 1)
    y = np.random.uniform(0,1)
    
    # Do the accept/reject comparison
    if y < p(xproposal)/g(xproposal):
        samples[accepted] = xproposal
        accepted += 1
    
    count +=1
    
print("Count", count, "Accepted", accepted)

# get the histogram info
hinfo = np.histogram(samples,50)

# plot the histogram
plt.hist(samples,bins=50, label=u'Samples');

# plot our (normalized) function
xvals=np.linspace(xmin, xmax, 1000)
plt.plot(xvals, hinfo[0][0]*p(xvals), 'r', label=u'p(x)')
plt.plot(xvals, hinfo[0][0]*g(xvals), 'k', label=u'g(x)')



# turn on the legend
plt.legend()
```

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/Monte%20Carol%20Method/image/M-reject-samp.png" width = "600" height = "400"/>


## 3. Important Sampling

**Different from reject sampling, import sampling does not has any rejection action. To replace with rejection action, important sampling adopt the approach of weighted sample.**  Specifically, suppose we want to evaluate expectations using samplings from a complicated probability distribution. We assume that

- $p(x)$ is hard to sample from but easy to evaluate
- $q(x)$ is easy to sample from
- $f(x)$ is a function we want to evaluate in expectation: $E_{p}[f(x)]$
- $q(x)>0$ whenever $p(x)>0$ or $q$ dominates $p$.

**Procedure (unnormalized important sampling)**

1. Draw $M$ samples $x_{m}$ from $q(x)$.
2. Determine weights $w_{m}$ for samples equal to the likelihood ratio $w_{m}=\frac{p(x_{m})}{q(x_{m})}$
3. Compute expectation as:

$$
\begin{equation}
E_{p}[f(x)] = \frac{1}{M}\sum_{m}f(x_{m})w_{m}
\end{equation}
$$

We call it unnormalized because these weights are likelihood are likelihood ratios, so there is no reason that they need to sum to 1. However, it gives us a first approximation to the true distribution.

Note that this does not give us sample from the target distribution but we can prove corretness for the expected value estimate

$$
\begin{equation}
\begin{split}
E_{p}[f(x)] &=\int f(x)p(x)\mathrm{d}x \\
&=\int f(x)\frac{p(x)}{q(x)}q(x)\mathrm{d}x \\
&\sim \frac{1}{M}\sum_{m}f(x_{m})\frac{p(x_{m})}{q(x_{m})} \\
&=\frac{1}{M}\sum_{m}f(x_{m})w_{m}
\end{split}
\end{equation}
$$

The key step is that third equality where we can approximate the integral assuming $x_{m}$ are drawn from $q(x)$ which they acyually are in the procedure.

**Normalized important sampling**

Here we no longer assume that we know $p(x)$ and instead only know it up to a constant factor $\hat{p}(x)=\alpha p(x)$. This is a common situation, such as when we want a conditional probability when we know the joint $p(x, e)$ but not the marginal $p(e)$. In this case, the samplilng procedure is

**Procedure**

1. Draw $M$ samples $x_{m}$ from $q(x)$.
2. Calculate ratios $r_{m}$ for samples equal to $r_{m}=\frac{\hat{p}(x_{m})}{q(x_{m})}$.
3. Compute expectation as 

$$
\begin{equation}
E_{p}[f(x)]=\frac{\sum_{m}f(x_{m})r_{m}}{\sum_{m}r_{m}}
\end{equation}
$$

We observe first that

$$
\begin{equation}
\begin{split}
E_{p}[r(x)] &= \int \frac{\hat{p}(x)}{q(x)}q(x)\mathrm{d}x \\
&=\int \hat{p}(x)\mathrm{d}x \\
&=\alpha
\end{split}
\end{equation}
$$

$$
\begin{equation}
\begin{split}
E_{p}[f(x)] &= \int f(x)p(x)\mathrm{d}x \\
&=\frac{1}{\alpha}\int f(x)\frac{\hat{p}(x)}{q(x)}q(x)\mathrm{d}x \\
&=\frac{\int f(x)r(x)q(x)\mathrm{d}x}{\int r(x)q(x)\mathrm{d}x} \\
&\sim \frac{\sum_{m} f(x_{m})r_{m}}{\sum_{m}r_{m}} \\
&\sum_{m}f(x_{m})w_{m}
\end{split}
\end{equation}
$$





In detail, we want to draw sample from $h(x)$, where a function whose integral or expectation we desire, is large. In the case of expecation, it would indeed be even better to draw more samples where $h(x)f(x)$ is large, where $f(x)$ is the pdf we are calculating the integral with respect to. I will show how importan sampling work below:

**Why improtant?** Often, in the computation of an expectation or other integral, the integrand has a very small value on a dominant fraction of the whole integration volume. If the points are chosen evenly in the integration volume, the small minority of the points close to the ‘peak’ give the dominant contribution to the integral.

**(Example: Expectation)**

$$
\begin{equation}
\mathbb{E}_{f}[h]=\int_{V}f(x)h(x)dx
\end{equation}
$$

Choose a distribution $g(x)$, which is close to the function $f(x)$, but which is simple enough so that it is possible to generate random $x$-values from this distribution. The integral can now be re-written as:

$$
\begin{equation}
\mathbb{E}_{f}[h]=\int h(x)g(x)\frac{f(x)}{g(x)} dx
\end{equation}
$$

Therefore if we choose random number $x_{i}$ from distribution $g(x)$, we obtain:

$$
\begin{equation}
\mathbb{E}_{f}[h(x)]=\lim_{N\rightarrow \infty}\frac{1}{N}\sum_{x_{i}\sim g(\cdot)}h(x_{i})\frac{f(x_{i})}{g(x_{i})}
\end{equation}
$$

Let $w(x_{i})=\frac{f(x_{i})}{g(x_{i})}$, the formulation can be rewritten:

$$
\begin{equation}
\mathbb{E}_{f}[h(x)]=\lim_{N\rightarrow \infty}\frac{1}{N}\sum_{x_{i}\sim g(\cdot)}h(x_{i})\omega(x_{i})
\end{equation}
$$

Now the variance(error) of monte carol is that:

$$
\begin{equation}
\widetilde{V}=\frac{V_{f}[h(x)]}{N}
\end{equation}
$$

where $N$ is the sample size.

With the important sampling this formula has now changed to

$$
\begin{equation}
\widetilde{V}=\frac{V_{g}[\omega(x)h(x)]}{N}
\end{equation}
$$

Our goal is to minimize the $V_{g}[\omega(x)h(x)]$.

As a somewhat absurd notion, this variance should be set to zero, if 

$$
\begin{equation}
\omega(x)h(x)=C\Rightarrow f(x)h(x)=Cg(x)
\end{equation}
$$

which leads to (since $g(x)$ is density thus we need normalization):

$$
\begin{equation}
g(x) = \frac{f(x)h(x)}{\int f(x)h(x)dx}=\frac{f(x)h(x)}{\mathbb{E}_{f}[h(x)]}
\end{equation}
$$

Actually, the expection is what we expect to estimate. Let's ignore the denominator, this formula tell us that to achieve low variance, we must have $g(x)$ large where the product $f(x)h(x)$ is large. Didirectly, maximizing the latter in some fashion was our original intuition.

Or from another perspective, $\frac{f(x)}{g(x)}$ ought to be large where $h(x)$ is large. This means that, as we say earlier, choose more samples near the peak.

In detail,  We have a $f$ that we might or might not know. We have a pdf $g$ which we choose to be higher than $f$ at the points where hh has peaks. Now what we are left to do is to sample from $g$, and this will give us an oversampling at the place hh has peaks, and thus we must correct this there by multiplying by weights $w=\frac{f}{g}<1$ in thse places.

Be careful to choose $g(x)$ appropriately, it should have thicker tails than $f$, or the ratio $\frac{f}{g}$ will be too big and count contribute too much in the tails. All of these considerations may be seen in the diagram below:

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/other/C681AABA-986F-4C87-9C8F-EB9566F5F689.png" width = "600" height = "400"/>

Another way of seeing this whole thing is that we will draw the sample from a proposal distribution and re-weight the integral appropriately so that the expectation with respect to the correct distribution is used. And since $\frac{f}{g}$ is flatter than $f$, the variance of $h\times\frac{f}{g}$ is smaller that the variance of $h\times f$ and therefore the error will be smaller for all N.

### Code of Important Sampling ( Example: $\int_{0}^{\pi}sin(x)xdx$ )

```python

from scipy import stats
from scipy.stats import norm

mu = 2;
sig = .7;

f = lambda x: np.sin(x)*x
infun = lambda x: np.sin(x)-x*np.cos(x)
p = lambda x: (1/np.sqrt(2*np.pi*sig**2))*np.exp(-(x-mu)**2/(2.0*sig**2))
normfun = lambda x: norm.cdf(x-mu, scale=sig)

# Range of integration 
xmax = np.pi
xmin = 0

# Number of draws
N = 1000

#Just Want to plot the function
x = np.linspace(xmin,xmax,1000)
plt.figure(figsize=(18,8))
plt.subplot(1,2,1)
plt.plot(x, f(x), 'b', label='Original $x\sin(x)$')
plt.plot(x, p(x), 'r', label='Important Sampling Function:')
plt.plot(x, np.ones(1000)/np.pi, 'k')
xis = mu + sig*np.random.randn(N,1)
plt.plot(xis, 1/(np.pi*p(xis)), '.', alpha=0.1)
plt.xlim([0, np.pi])
plt.ylim([0, 2])
plt.xlabel('x')
plt.legend()

## VANILLA MONTE CAROL
Ivmc = np.zeros(1000)
for k in np.arange(0, 1000):
    x = np.random.uniform(low=xmin, high=xmax, size=N)
    Ivmc[k] = (xmax-xmin)*np.mean(f(x))
    
print('Mean basic MC estimate:', np.mean(Ivmc))
print('Standard deviation of our estimates:', np.std(Ivmc))

## IMPORTANCE SAMPLING, choose gaussian so it is 
## similar to the original functions 

Iis = np.zeros(1000)
for k in np.arange(0, 1000):
    xis = mu + sig * np.random.randn(N,1)
    xis = xis[(xis<xmax) & (xis>xmin)]
    
    # normalization for gaussian from 0 to pi
    normal = normfun(np.pi)-normfun(0)
    Iis[k] = np.mean(f(xis)/p(xis))*normal

print('Mean important sampling MC estimate:', np.mean(Iis))
print('Standard deviation of our estimates:', np.std(Iis))

plt.subplot(1,2,2)
plt.hist(Iis, 30, histtype='step', label='Importance Sampling')
plt.hist(Ivmc, 30, color='r', histtype='step', label='Vanilla')
plt.grid('on')
plt.legend()

```

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/Monte%20Carol%20Method/image/import-samp.png" width = "600" height = "400"/>

## 4. Markov Chain Monte Carol(MCMC)

### 4.1 What's the Markov Chain

**([Definition](https://en.wikipedia.org/wiki/Markov_chain):)** A Markov Chain is a stochastic process that satisfies the Markov property. In other words, a sequence of random variable taking value in state space is called a Markov Chain if the probability of the next step only depends on the current state(from [Lecture 9](https://am207.github.io/2017/wiki/markov.html)).

Using the notation of transition probabilities to define the probability of going from state $x$ ot state $y$ as $T(x\vert y)$, we can write this mathematically:

$$
\begin{equation}
T(x_{n+1}\vert x_{n},...,x_{1})=T(x_{n+1}\vert x_{n})
\end{equation}
$$

### 4.2 Some Markov Chains

**Homogenous Markov Chain:** A chain is homogeneous at step tt if the transition probabilities are independent of tt. Thus the evolution of the Markov chain only depends on the previous state with a fixed transition matrix.

**Irreducible Markov Chain:** Every state is accessible in a finite number of steps from another state. That is, there are no absorbing states. In other words, one eventually gets everywhere in the chain.(*example:* Consider as an example surfing the web. We do want to reach all parts of the web so we dont want to be trapped into an subset.)

**Recurrent:** States visited repeatedly are recurrent: positive recurrent if time-to-return is bounded and null recurrent otherwise. Harris recurrent if all states are visited infinitely often as $t\rightarrow \infty$.

**Aperiodic:** There are no deterministic loops. This would be bad in our web example as well as we would be stuck in a loop at some pages.

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/other/0D4DDB79-114A-4DBB-BDC4-A6E16601B700.png" width = "600" height = "400"/>

### 4.3 Stationary Markov Chain

**(Formal Definition of Stationarity):** A staionary Markov Chain produces the same marginal distribution when multiplied by the transition matrix. That is 

$$
\begin{equation}
sT = s \text{ or }\sum_{i}s_{i}T_{ij}=s_{j}
\end{equation}
$$

In the case of continuous state space, which are the ones we encounter in sampling, if the transition kernel T is defined so that

$$
\begin{equation}
\int dx_{i}s(x_{i})T(x_{i+1}\vert x_{i})=s(x_{i+1})
\end{equation}
$$

then 

$$
\begin{equation}
\int dxs(x)T(y\vert x)=s(y)
\end{equation}
$$

**Ergodicity**: Aperiodic, irreducible, positive Harris recurrent markov chains are ergodic, that is, in the limit of infinite (many) steps, the marginal distribution of the chain is the same(the probability of all state remain stable, nonzero and is independent with the initial position). This means that if we take largely spaced about samples from a stationary markov chain, we can draw independent samples.

$$
\begin{equation}
\int g(x)f(x)dx=\frac{1}{N}\sum_{j=B+1}^{B+N}g(x_{j})
\end{equation}
$$

Here B is called the burin (which comes from the approach to stationarity after a while) and T is called the thinning (which comes from ergodicity). So we have this “ergodic” law of large numbers.

([摘抄慕课网笔记](https://mooc.guokr.com/note/15627/))在满足一定条件的情况下，马尔可夫过程将收敛至一个均衡。这是一个统计均衡，在每种状态下的概率是固定不变的，但事物将依旧在各个状态间转移。

马尔可夫过程收敛到均衡的四个条件：

一、可能的状态数量是有限的。

二、转移概率固定不变。

三、从任意一个状态能够变到任意其他一个状态。有可能不是从状态A直接变到状态C，而是先变到状态B再变到C，但只要有路径从状态A变成状态C就行。

四、过程不是简单循环。比如不能是从全A变到全B，然后又自动从全B变到全A。

马尔可夫收敛定理（Markov Convergence Theorem）：如果满足上述四个条件，一个马尔科夫过程将收敛到一个均衡状态，且此均衡唯一。

**只要转移概率不变，那么初始状态、历史过程、中途干预都不重要，最后必将达到那个唯一的均衡。换句话说，马尔科夫链最后达到的均衡与初始状态，转移过程以及中途干预无关。** 

```python

import scipy.stats as st

def target(lik, prior, n, h, theta):
    if theta < 0 or theta > 1:
        return 0
    else:
        return lik(n, theta).pmf(h)*prior.pdf(theta)

def mh_coin(niters, n, h, theta, lik, prior, sigma):
    samples = [theta]
    while len(samples) < niters:
        theta_p = theta + st.norm(0, sigma).rvs()
        rho = min(1, target(lik, prior, n, h, theta_p)/target(lik, prior, n, h, theta ))
        u = np.random.uniform()
        if u < rho:
            theta = theta_p
        samples.append(theta)
    return samples

n = 100
h = 61
lik = st.binom
prior = st.beta(10, 10)
sigma = 0.05
niters = 100

sampless = [mh_coin(niters, n, h, theta, lik, prior, sigma) for theta in np.arange(0.1, 1, 0.2)]

# Convergence of multiple chains

for samples in sampless:
    plt.plot(samples, '-o')
plt.xlim([0, niters])
plt.ylim([0, 1]);
```

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/Monte%20Carol%20Method/image/mcmc-recure.png" width = "600" height = "400"/>

### 4.3 Markov Chain and Monte Carol Methods

A irreducible (goes everywhere) and aperiodic (no cycles) markov chain will converge to a stationary markov chain. It is the marginal distribution of this chain that we want to sample from, and which we do in metropolis (and for that matter, in simulated annealing).

As we can see above, to find stationary distribution, we need to solve an eigenvector proble. This can be hard.

However, A sufficient, but not necessary, condition to ensure that s(x)s(x) is the desired stationary distribution is the already seen reversibility condition, also known as detailed balance:

$$
\begin{equation}
\int dxs(x)t(y\vert x)=s(y)\int dxT(x\vert y)
\end{equation}
$$

which gives back us back the stationarity condition from above.

Thus we want to design us samplers which satisfy detail balance.

### 4.4 Metropolis Hasting Algorithm(MH Algorithm)

**(why do we need Metropolis Hasting Algorithm?)** We've learnt how to do the inverse transform and how to use rejection sampling with a majority function. So why do we use these methods to sample a ditribution? ** inefficient as dimensions increased.** In other words, dimension curse. **How do we understand this point?**

In generally, we want to calculate the expectation of distribution as sample average, however, as dimension of space increased, majorizing in multiple dimensions can have us spending a lot of time in tail dimension because you leave more and more space out.  If inverse tranform and reject sampling methods are adopted, then it will boost inefficient.

In multiple dimensions, volumns get smaller and smaller, that's the curse of dimension. This concept can be shown as:

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/other/D5B3AE61-1E06-4445-AE57-9263315A536E.png" width = "600" height = "400" alt="Important Sampling"/>

where the centre-partitions combination to an integral goes from 1/3rd to 1/27th. Now suppose the mode of the distibution is contained in this partition: then its contribution to the integral is going down with dimensions.

As the centre volume decreases, the outer volume increases, but this is in distribution tails, so we dont get much of a contribution from there either:

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/other/259532EC-97B2-4409-99FD-C2293E93D528.png" width = "600" height = "400" alt="Important Sampling"/>

It is the neighborhood between these extremes, called the typical set which our sampler must explore well. And to get a good rejection sampling majorizer for this becomes hard.

**The idea of MH Algorithm**

1. Use a proposal distribution to propose s step
2. Then we calculate the pdf at that step, and compare it to the one at that previous step.
3. If the probability increased we accept. If the probability decreased, we accept the some of time, based on the ratio of the new probability to the old one.
4. We accumulate our samplees, as we now trying to sample a distribution.

**MH Algorithm**

1. initialize $x^{(0)}$.
2. Draw $\mu \sim U(0,1)$.
3. Draw propose $x^{(\ast)} \sim q(x^{(\ast)}\vert x)$.
4. If $\mu<\min(1, \frac{s(x^{(\ast)})q(x\vert x^{(\ast)})}{s(x)q(x^{(\ast)}\vert x)})$, $x^{(i+1)}=x^{(\ast)}$. Else, $x^{(i+1)}=x$
5. back step 2 for loop.

**Why does MH algorithm work?**

To prove if MH algorithm work is to prove MH algorithm satisfied detail balance condition.

Transition Kernel $K(x\rightarrow x^{\ast})$ includes the joint density of the following:

  1. Propose $x^{(\ast)}$ from the $q(x^{(\ast)} \vert x)$.
  2. the accept $x^{(\ast)}$ with the ratio $\alpha(x^{(\ast)}, x)=\min(1,\frac{s(x^{(\ast)})q(x\vert x^{(\ast)})}{s(x)q(x^{(\ast)}\vert x)})$

To verify the detail balance condition:

$$
\begin{aligned}
s(x^{*})q(x^{*}\vert x)\alpha(x^{*},x) &= s(x^{*})q(x^{*}\vert x)\min(1,\frac{s(x^{*})q(x\vert x^{*})}{s(x)q(x^{*}\vert x)}) \\
&= \min(s(x)q(x^{*}\vert x), s(x^{*})q(x\vert x^{*})) \\
&= s(x)q(x\vert x^{*})\min(1,\frac{s(x)q(x^{*}\vert x)}{s(x^{*})q(x\vert x^{*})}) \\
&= s(x)q(x\vert x^{*})\alpha(x,x^{*})
\end{aligned}
$$

**Code of Metropolis Hasting Algorithm**

```python

def metropolis_hastings(p,q, qdraw, nsamp, xinit):
    samples=np.empty(nsamp)
    x_prev = xinit
    accepted=0
    for i in range(nsamp):
        x_star = qdraw(x_prev)
        p_star = p(x_star)
        p_prev = p(x_prev)
        pdfratio = p_star/p_prev
        proposalratio = q(x_prev, x_star)/q(x_star, x_prev)
        if np.random.uniform() < min(1, pdfratio*proposalratio):
            samples[i] = x_star
            x_prev = x_star
            accepted +=1
        else:#we always get a sample
            samples[i]= x_prev
            
    return samples, accepted

# target function
f = lambda x: 0.554*x*np.exp(-(x/1.9)**2)

x = np.linspace(0,10,100)
plt.plot(x, f(x), 'r')
plt.grid('on')
plt.title('The target function')

```

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/Monte%20Carol%20Method/image/mh.png" width = "600" height = "400"/>

```python
from scipy.stats import gamma

t=10.0

def gammapdf(x_new, x_old):
    return gamma.pdf(x_new, x_old*t, scale=1/t)

def gammadraw(x_old):
    return gamma.rvs(x_old*t,scale=1/t)

x_init = np.random.uniform()
samps, acc = metropolis_hastings(f, gammapdf, gammadraw, 100000, x_init)

# plot our sample histogram
plt.hist(samps,bins=100, alpha=0.4, label=u'MCMC distribution', normed=True) 
somesamps=samps[0::20000]
for i,s in enumerate(somesamps):
    xs=np.linspace(s-3, s+3, 100)
    plt.plot(xs, gamma.pdf(xs,s*t,scale=1/t),'k', lw=1)
xx= np.linspace(0,10,100)
plt.plot(xx, f(xx), 'r', label=u'True distribution') 
plt.legend()
plt.xlim([0,10])
plt.show()
print("starting point was ", x_init)

```

<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/Monte%20Carol%20Method/image/mh-1.png" width = "600" height = "400"/>

## 5. Gibbs Sampling

### 5.1 The idea of Gibbs Sampling

The idea of gibbs as a markov chain in which the transition matrix can be obtained as the kernel of the an integral fixed point equation by sampling alternatively from two conditionals.

Gibbs determined the energy states of gases at equilibrium by cycling through all the particles, drawing from each one of them conditionally given the enerygy levels of the others, taking the time average. 

Now, suppose you have a density function of two variables $(x,y)$. You wish to sample from this density.

The definition of the X marginal is 

$$
\begin{equation}
f_{X}(x)=\int f_{XY}(x,y)dy
\end{equation}
$$

We reformulate the above formula:

$$
\begin{equation}
f_{X}(x)=\int f_{XY}(x,y)dy=\int f(x\vert y)f(y)dy=\int f(x\vert y)dy\int f(y\vert x^{'})f(x^{'})dx^{'}
\end{equation}
$$

Thus

$$
\begin{equation}
f(x)=\int h(x,x^{'})f(x^{'})dx^{'}
\end{equation}
$$

where

$$
\begin{equation}
h(x,x^{'})=\int f(x\vert y)f(y\vert x^{'})dy
\end{equation}
$$

Now consider an iterative scheme in which the “transition kernel” $h(x,x′)$ is used to create a proposal for metropolis-hastings moves. This looks like:

$$
\begin{equation}
f(x_{t})=\int h(x_{t},x_{t-1})f(x_{t-1})dx_{t-1}
\end{equation}
$$

which is the equation of stationary distribution.

The big idea, then, here, as in the case of markov chains, is that the above equation can be thought of as a fixed-point integral equation, and thus we can think of an iterative process which at first does not satisy this condition but then does as time goes on and we reach stationarity.

Similarly here, if we draw $y$, from the conditional $f(y\vert x)$ and then $x$ again from $f(x\vert y)$ we will be able to get the marginal distribution of $x$. Symmetrically we can get the marginal for $y$.

Now, if I can draw from the $x$ marginal, and the $y\vert x$ conditional, i can draw from the $x,y$ joint, and I am done.

### 5.2 Gibbs Sampling Algorithm

- Given a string sampling $(x_{1},y_{1},z_{1})^{T}$.
- You want to sample: ${(x_{2},y_{2},z_{2})^{T}, (x_{3},y_{3},z_{3})^{T},...,(x_{N},y_{N},z_{N})^{T}}\sim P(x,y,z)$.
- Then the algorithm goes:

$$
\begin{aligned}
x_{2}&\sim P(x\vert y_{1},z_{1}) \\
y_{2}&\sim P(y\vert x_{2},z_{1}) \\
z_{2}&\sim P(z\vert y_{2},x_{2}) \\
&... \\
x_{3}&\sim P(x\vert y_{2},z_{2}) \\
y_{3}&\sim P(y\vert x_{3},z_{2}) \\
z_{3}&\sim P(z\vert x_{3},y_{3}) \\
\end{aligned}
$$

### 5.3 Gibbs and MH Algorithm

Gibbs is the extension of MH Algorithm in high dimension and also is a special case of MH Algorithm without rejection. I will show you the proof.

Look at the M-H acceptance ratio:

- Let **x**=$x_{1},...,x_{D}$.
- When sampling $k$th component, $q_{k}(x^{*}\vert x)=\pi(x_{k}^{*}\vert x_{-k})$.
- When sampling $k$th component, $x^{*}_{-k}=x_{-k}$.

$$
\begin{equation}
\frac{\pi(x^{*})q(x\vert x^{*})}{\pi(x)q(x^{*}\vert x)}= \frac{\pi(x^{*})q_{k}(x\vert x^{*}_{-k})}{\pi(x)q(x^{*}_{k}\vert x_{-k})} = \frac{\pi(x^{*}\vert x^{*}_{-k})q_{k}(x\vert x^{*}_{-k})}{\pi(x_{k}\vert x_{-k})q(x^{*}_{k}\vert x_{-k})}=1
\end{equation}
$$

**Code of Gibbs sampling**

$$
\begin{equation}
f(x,y)=x^{2}\text{exp}(-xy^{2}-y^{2}-2*y-4*x)
\end{equation}
$$

```python

func = lambda x,y: x**2*np.exp( -x*y**2 - y**2 + 2*y - 4*x )

numgridpoints=400
x = np.linspace(0,2,numgridpoints)
y = np.linspace(-1,2.5,numgridpoints)
xx,yy = np.meshgrid(x,y)
zz = np.zeros((numgridpoints,numgridpoints))
for i in np.arange(0,numgridpoints):
    for j in np.arange(0,numgridpoints):
        zz[i,j]=func(xx[i,j],yy[i,j])
        
plt.contourf(xx,yy,zz)

```
<img src="https://raw.githubusercontent.com/Gwan-Siu/BlogCode/master/Monte%20Carol%20Method/image/gibbs.png" width = "600" height = "400"/>